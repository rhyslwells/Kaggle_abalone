{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n---\ntitle: \"Regression with an Abalone Dataset\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict the age of abalone from various physical measurements.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/focal/2021-05-17\"))\n \nremotes::install_version(\"ranger\", quiet = TRUE)\nremotes::install_github(\"mayer79/outForest\", quiet = TRUE)\n\nsuppressPackageStartupMessages({\nlibrary(tidyverse) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \n# library(extrasteps)\n\nlibrary(bonsai) # interface with lightgbm\nlibrary(poissonreg) # interface with poisson regression glmnet\nlibrary(brulee) # interface with Torch deep learning    \n\n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(cowplot::theme_minimal_grid())\n\n```\n\n\n## Interchangeability\n\nI prefer to be able to run the same code locally and on a Kaggle kernel.\n\n```{r}\n#| label: interchangeability\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s4e4/\"\n} else {\n  path <- str_c(here::here(\"data\"),\"/\")\n}\n\npath\n\n```\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\npreprocessor <- function(dataframe) {\n  dataframe %>%\n    janitor::clean_names() %>% \n    mutate(across(c(where(is.character)), ~ as.factor(.x)),)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   show_col_types = TRUE) %>%\n\n  preprocessor() %>%\n  filter(between(height, 0.0001, 0.3))\n\nfeatures <- raw_df %>%\n  select(-id, -rings) %>%\n  names()\n\nraw_df <- raw_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\ncompetition_df <- read_csv(str_c(path, \"test.csv\"),\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n            competition_df %>% mutate(source = \"test\"))\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n\nI initially implemented an Outlier section, but went back to preprocess to remove the high and low `height` figures from training.\n\nThere are a couple outlier `height` values in test as well.\n\nOther kernels make some effort to impute zero `height` figures.\n\n\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 12\n#| fig.width: 12\n\nraw_df %>% \n  select(all_of(num_features), rings) %>% \n  pivot_longer(-rings,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = ggplot2::cut_number(rings,5))) +\n  geom_histogram(alpha = 0.6, bins = 50) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 3) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(color = NULL, fill = \"Rings\",\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\nraw_df %>% \n  select(all_of(num_features), rings) %>% \n  pivot_longer(-rings,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(sample = value, color = ggplot2::cut_number(rings,5))) + \n  stat_qq(show.legend = FALSE) + \n  stat_qq_line() +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 3) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())+\n  labs(color = NULL, \n       title = \"Numeric Feature Q-Q Distributions\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\nmy_bin <- function(data, mapping, ..., low = \"#132B43\", high = \"#56B1F7\") {\n  ggplot(data = data, mapping = mapping) +\n    geom_bin2d(...) +\n    scale_fill_gradient(low = low, high = high)\n}\n\nraw_df %>%\n  select(all_of(num_features), rings, sex) %>% \n  GGally::ggpairs(\n    mapping = aes(color = sex),\n    lower = list(\n      combo = GGally::wrap(\"facethist\", binwidth = 1)\n    )\n  )\n\n```\n\n## Outforest\n\n`outForest` is a multivariate anomaly detection method. Each numeric variable is regressed onto all other variables using a random forest. If the scaled absolute difference between observed value and out-of-bag prediction is larger than a prespecified threshold, then a value is considered an outlier.\n\nA benefit of this technique is noise reduction, at least at the most anomylous observations.  A drawback is of applying it at this point to the training data and the cross validation folds is that the CV out of sample estimates of error are smaller than we would see with raw.\n                   \nComputationally, it is faster to run it here.                   \n\n```{r}\n#| label: outforest anomaly detection and interpolation\n                   \noutforest_model <- outForest::outForest(\n  data = raw_df,\n  formula =  formula(paste0(str_c(features,  collapse = \" + \"), \"~ rings\")),\n  max_prop_outliers = 0.0005,\n  threshold = 3,\n  impute_multivariate_control = list(\n    pmm.k = 3L,\n    num.trees = 250L,\n    maxiter = 3L\n  ),\n  allow_predictions = TRUE,\n  verbose = 0\n)\n\nplot(outforest_model, what = \"scores\")\nplot(outforest_model, what = \"counts\")\n\noutforest_preds <- predict(outforest_model, newdata = raw_df)\n\ntrain_df <- outForest::Data(outforest_preds) %>% as_tibble()\n\n```\n\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\nraw_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n```\n\n## Counts of Distinct\n               \n```{r}\n#| label: counts of distinct\n               \nraw_df %>% \n  summarize(\n    across(all_of(features), n_distinct)\n  ) %>%\n  pivot_longer(everything(),\n               names_to = \"feature\",\n               values_to = \"Count of distinct\") %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\nall_df %>%\n    select(all_of(features), source) %>% \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>%\n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>%\n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n## Pairwise Correlations\n                   \n`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. \n                         \n\n```{r}\n#| label: pairwise correlations\n# Leave blank on no significant coefficient\n\ncorr <- raw_df %>%\n  select(all_of(num_features), rings) %>%\n  cor()\n\np.mat <-raw_df %>%\n  select(all_of(num_features), rings) %>%\n  ggcorrplot::cor_pmat() \n\nggcorrplot::ggcorrplot(\n    corr,\n    hc.order = TRUE,\n    lab = TRUE,\n    type = \"lower\",\n    insig = \"blank\",\n    p.mat = p.mat\n  ) +\n  labs(title = \"Pairwise Correlations Training Set\")\n\nggcorrplot::ggcorrplot(\n   competition_df %>%\n    select(all_of(num_features)) %>%\n    cor(),\n    hc.order = TRUE,\n    lab = TRUE,\n    type = \"lower\",\n    insig = \"blank\",\n    p.mat = competition_df %>%\n  select(all_of(num_features)) %>%\n  ggcorrplot::cor_pmat() \n  ) +\n  labs(title = \"Pairwise Correlations Competition Set\")\n\n```      \n\n\n## Target\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 12\n\nraw_df %>% \n ggplot(aes(rings)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"Outcome: Rings\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n                              \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n## Recipe\n                   \nWe will tune each modeling algorithm against 10 fold cross validation, stratified for the outcome.\n                 \n\n```{r}\n#| label: recipe\n#| fig.height: 16\n#| fig.width: 12\n\nfolds <- vfold_cv(raw_df, \n                  v = 11,\n                  repeats = 1,\n                  strata = rings)\n\nrec <- recipe(\n    \n    formula(paste0(\"rings ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = raw_df \n  ) %>%\n step_ratio(shell_weight, denom = denom_vars(length)) %>%                   \n step_novel(all_nominal_predictors()) %>% \n step_nzv(all_numeric_predictors())\n\nrec\n                  \n```\n\n## Metric\n\nRoot Mean Squared Log Error\n\n\n```{r}\n#| label: custom RMSLE metric\n\nrmsle_impl <- function(truth, estimate, case_weights = NULL) {\n        sqrt(mean((log(abs(estimate) + 1) - log(abs(truth) + 1))^2))\n}\n\nrmsle_vec <- function(truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {\n  check_numeric_metric(truth, estimate, case_weights)\n\n  if (na_rm) {\n    result <- yardstick_remove_missing(truth, estimate, case_weights)\n\n    truth <- result$truth\n    estimate <- result$estimate\n    case_weights <- result$case_weights\n  } else if (yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n\n  rmsle_impl(truth, estimate, case_weights = case_weights)\n}\n\nrmsle <- function(data, ...) {\n  UseMethod(\"rmsle\")\n}\n\nrmsle <- new_numeric_metric(rmsle, direction = \"minimize\")\n\nrmsle.data.frame <- function(data, truth, estimate, case_weights = NULL, na_rm = TRUE, ...) {\n  \n  numeric_metric_summarizer(\n    name = \"rmsle\",\n    fn = rmsle_vec,\n    data = data,\n    truth = !!enquo(truth),\n    estimate = !!enquo(estimate),\n    na_rm = na_rm,\n    case_weights = !!enquo(case_weights)\n  )\n  \n}\n\nmetrics <- metric_set(rmsle)\n\n\n```\n\n## LightGBM \n\n```{r}\n#| label: lightgbm engine\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 2500L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n    loss_reduction = 0\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = future::availableCores()\n             ) %>%\n  set_mode(mode = \"regression\") \n\nboost_tree_lgbm_spec\n\n```\n\n                                                                     \n\n```{r}\n#| label: lgbm fit \n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\nwf <- workflow(rec,\n               boost_tree_lgbm_spec) \n\nset.seed(42)\n\nctrl <- finetune::control_sim_anneal(     \n     verbose = FALSE,\n     verbose_iter = TRUE,\n     parallel_over = \"everything\",\n     save_pred = TRUE,\n     save_workflow = TRUE)\n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n      min_n = min_n(range = c(10, 40)),\n      tree_depth = tree_depth(range = c(6, 40)),\n      learn_rate = learn_rate(range = c(-1.5, -2.5)),\n      num_leaves = num_leaves(range = c(100, 250))\n   ) %>%\n   dials::finalize(raw_df)                   \n                   \nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metrics,\n  param_info = param)\n\nlgbm_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 18,\n  initial = burnin,\n  control = ctrl,\n  metrics = metrics,\n  param_info = param) \n\nshow_best(lgbm_rs)                   \n                                     \nautoplot(lgbm_rs)                   \n\ntictoc::toc()                    \n```\n                   \n## Poisson GLMnet                   \n\n```{r}\n#| label: poisson reg  \n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\n                   \nall_cores <- parallelly::availableCores()\n\nfuture::plan(\"multisession\", workers = all_cores)                    \n\npoisson_rec <- rec %>%             \n  step_poly(all_numeric_predictors()) %>%   \n  step_dummy_multi_choice(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\npoisson_reg_glmnet_spec <-\n  poisson_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine('glmnet')\n\nwf <- workflow(poisson_rec  ,\n               poisson_reg_glmnet_spec) \n\nset.seed(42)\n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n           mixture = mixture(),\n           penalty = penalty(range = c(-4, -2))\n        ) %>%\n   dials::finalize(raw_df)\n\npoisson_rs <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = 10,\n  control = ctrl,\n  metrics = metrics,\n  param_info = param\n)\n\nautoplot(poisson_rs) +\n  labs(title = \"Poisson GLMnet hyperparameter tuning\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")      \n                   \nshow_best(poisson_rs)                          \n                   \nfuture::plan(\"sequential\")                                   \ntictoc::toc()                    \n```\n                   \n## XGBoost                \n\n```{r}\n#| label: xgboost\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\n\nall_cores <- parallelly::availableCores()\n\nfuture::plan(\"multisession\", workers = all_cores) \n                   \n                   \nxgb_spec <-\n  boost_tree(\n    trees = 3000,\n    tree_depth = 7,\n    learn_rate = 0.03,\n    min_n = 25) %>%\n  set_engine(engine = \"xgboost\") %>%\n  set_mode(mode = \"regression\")\n\nwf <- workflow(rec %>%\n       step_dummy_multi_choice(all_nominal_predictors())  ,\n               xgb_spec) \n\nxgb_rs <- fit_resamples(\n  wf,\n  resamples = folds,\n  control = ctrl,\n  metrics = metrics)\n\ncollect_metrics(xgb_rs)                   \n                   \nfuture::plan(\"sequential\") \n                   \ntictoc::toc()                    \n```      \n                   \n## Torch                   \n\n```{r}\n#| label: brulee\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\n\nmlp_brulee_spec <-\n  mlp(\n    hidden_units = 18,\n    epochs = 1500,\n    learn_rate = 0.1) %>%\n  set_engine('brulee',\n            stop_iter = 100) %>%\n  set_mode('regression')\n\nwf <- workflow(rec %>%\n  step_dummy_multi_choice(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())  ,\n               mlp_brulee_spec) \n\nset.seed(42)\n\nbrulee_rs <- fit_resamples(\n  wf,\n  resamples = folds,\n  control = ctrl,\n  metrics = metrics)\n\ncollect_metrics(brulee_rs)\n\ntictoc::toc()                    \n``` \n                   \n## SVM\n                   \n```{r}\n#| label: svm\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\n\nsvm_poly_kernlab_spec <-\n  svm_poly(\n    cost = 6.66,\n    degree = 2,\n    scale_factor = 0.003,\n    margin = 0.126\n  ) %>%\n  set_engine('kernlab') %>%\n  set_mode('regression')\n\nwf <- workflow(rec %>%\n  step_dummy_multi_choice(all_nominal_predictors()) %>%               \n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors()) ,\n               svm_poly_kernlab_spec)\n\nset.seed(42)\n\nsvm_rs <- fit_resamples(\n  wf,\n  resamples = folds,\n  control = ctrl,\n  metrics = metrics)\n\ncollect_metrics(svm_rs)\n\ntictoc::toc()                    \n```                  \n                   \n\n# Ensemble Stacks\n\n```{r}\n#| label: stacking ensemble\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\n\nall_cores <- parallelly::availableCores()\n\nfuture::plan(\"multisession\", workers = all_cores) \n\n\nens <- stacks::stacks() %>%\n  stacks::add_candidates(lgbm_rs) %>%\n  stacks::add_candidates(poisson_rs) %>%    \n  stacks::add_candidates(xgb_rs) %>%\n  stacks::add_candidates(brulee_rs) %>%    \n  stacks::add_candidates(svm_rs) %>%\n  stacks::blend_predictions(\n    metric = metrics,\n    penalty = c(seq(0.0001, 0.01, 0.0002)),\n    control = tune::control_grid(allow_par = TRUE)\n  ) \n\nautoplot(ens)\n\nautoplot(ens, \"weights\")\n\nensemble <- stacks::fit_members(ens)\n                   \nfuture::plan(\"sequential\") \n```    \n                   \n \n                   \n# The moment of Truth\n\n```{r}\n#| label: performance checks\n#| warning: false\n#| message: false\n\n\npredict(ensemble, raw_df) %>%\n  bind_cols(raw_df) %>%\n  rmsle(rings, .pred)\n\npredict(ensemble, raw_df) %>%\n  bind_cols(raw_df) %>%\n  ggplot(aes(rings, .pred - rings, color = sex), shape = 21, alpha = 0.05) + \n                   geom_jitter() +\n  labs(title = \"Residuals\")\n                   \n```\n                   \n## Submission\n                   \nThis submission will be without the OutForest anomaly removal.                    \n                   \n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n                   \nsubmit_df <- predict(ensemble, competition_df) %>%\n  bind_cols(competition_df) %>%\n  mutate(Rings = if_else(.pred > 0, round(.pred), 1)) %>%\n  select(id, Rings)\n\nhead(submit_df)  %>% \n     knitr::kable()      \n                   \nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n  \n\n```","metadata":{"_uuid":"e4b8d2ff-5716-4137-8f2f-880681626469","_cell_guid":"6aae7d0e-67c6-478e-a2bb-00cb00bea3ea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}