{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":7993471,"sourceType":"datasetVersion","datasetId":4705988},{"sourceId":8093830,"sourceType":"datasetVersion","datasetId":4729127},{"sourceId":170678041,"sourceType":"kernelVersion"},{"sourceId":172150510,"sourceType":"kernelVersion"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. INTRODUCTION\n<center>\n<img src=\"https://camanchaca.cl/en/wp-content/uploads/2020/07/Abalo%CC%81n-marino-a-la-parrilla.jpg\" width=1800 height=800 />\n</center>","metadata":{}},{"cell_type":"markdown","source":"**PROJECT DESCRIPTION**\n\n<font size=\"3\">Predicting the age of abalone from physical measurements.  The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope</font>\n\n**PHYSICAL ATTRIBUTES**\n1. **SEX:** <font size=\"3\"> Male/Female/Infant</font>\n2. **LENGTH:** <font size=\"3\"> Longest shell measurement</font>\n3. **DIAMETER:** <font size=\"3\"> Diameter of the Abalone</font>\n4. **HEIGHT:** <font size=\"3\"> Height of the Abalone</font>\n5. **WHOLE WEIGHT:** <font size=\"3\"> Weight of the whole abalone</font>\n6. **SHUCKED WEIGHT:** <font size=\"3\"> Weight of the meat</font>\n7. **VISCERA WEIGHT:** <font size=\"3\"> Gut Weight - Interal Organs</font>\n8. **SHELL WEIGHT:** <font size=\"3\"> Shell Weight after drying</font>\n9. **RINGS:** <font size=\"3\"> Number of rings +1.5 gives Age of the Abalone</font>\n\n**EVALUATION METRIC:** <font size=\"3\">RMSLE: Root Mean Sqraed Error on Log Transformed Target</font>\n\n**ORIGINAL DATA:** The original data is sourced from the [site](https://archive.ics.uci.edu/dataset/1/abalone)\n","metadata":{}},{"cell_type":"markdown","source":"# 2. IMPORTS","metadata":{}},{"cell_type":"code","source":"                                                            import sys\nassert sys.version_info >= (3, 5)\n\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\nimport numpy as np\nimport os\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\ntqdm_notebook.get_lock().locks = []\nfrom prettytable import PrettyTable\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom copy import deepcopy\nfrom functools import partial\nfrom itertools import combinations\n\nfrom sklearn.cluster import KMeans\n# !pip install yellowbrick\n# from yellowbrick.cluster import KElbowVisualizer\n\nimport random\nfrom random import uniform\nimport gc\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom scipy import stats\nimport statsmodels.api as sm\nimport math\nimport seaborn as sns\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n!pip install optuna\n!pip install cmaes\nimport optuna\nimport xgboost as xgb\n# !pip install catboost\n!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\nimport lightgbm as lgb\n!pip install category_encoders\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, RidgeCV, ElasticNetCV\nfrom sklearn.linear_model import TheilSenRegressor, RANSACRegressor, HuberRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor,ExtraTreesRegressor,GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom catboost import CatBoost, CatBoostRegressor,CatBoostClassifier\nfrom catboost import Pool\nfrom sklearn.neighbors import KNeighborsRegressor\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.pandas.set_option('display.max_columns',None)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-17T10:59:20.034765Z","iopub.execute_input":"2024-04-17T10:59:20.035983Z","iopub.status.idle":"2024-04-17T11:00:08.661910Z","shell.execute_reply.started":"2024-04-17T10:59:20.035937Z","shell.execute_reply":"2024-04-17T11:00:08.660313Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.2)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\nCollecting cmaes\n  Downloading cmaes-0.10.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from cmaes) (1.26.4)\nDownloading cmaes-0.10.0-py3-none-any.whl (29 kB)\nInstalling collected packages: cmaes\nSuccessfully installed cmaes-0.10.0\n\nUsage:   \n  pip install [options] <requirement specifier> [package-index-options] ...\n  pip install [options] -r <requirements file> [package-index-options] ...\n  pip install [options] [-e] <vcs project url> ...\n  pip install [options] [-e] <local project path> ...\n  pip install [options] <archive url/path> ...\n\nno such option: --install-option\nRequirement already satisfied: category_encoders in /opt/conda/lib/python3.10/site-packages (2.6.3)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.2.2)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.11.4)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.14.1)\nRequirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (2.2.1)\nRequirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.5.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2023.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category_encoders) (3.1.1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.1 Load Data","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\ntest=pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv')\nsubmission=pd.read_csv(\"/kaggle/input/playground-series-s4e4/sample_submission.csv\")\n\noriginal=pd.read_csv(\"/kaggle/input/playgrounds4e04originaldata/Original.csv\")\n\ntrain_copy=train.copy()\ntest_copy=test.copy()\n\n# Tag Orignal\noriginal[\"original\"]=1\ntrain[\"original\"]=0\ntest[\"original\"]=0\n\ntrain.drop(columns=[\"id\"],inplace=True)\ntest.drop(columns=[\"id\"],inplace=True)\noriginal.drop(columns=[\"id\"],inplace=True)\n\ntrain=train.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\ntest=test.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\n\ntrain=pd.concat([train,original],axis='rows')\ntrain.reset_index(inplace=True,drop=True)\n\ndevice='cpu'\n\ntarget='Rings'\n\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-17T11:00:08.664606Z","iopub.execute_input":"2024-04-17T11:00:08.665620Z","iopub.status.idle":"2024-04-17T11:00:09.075321Z","shell.execute_reply.started":"2024-04-17T11:00:08.665563Z","shell.execute_reply":"2024-04-17T11:00:09.073126Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  Viscera_weight  \\\n0   F   0.550     0.430   0.150        0.7715          0.3285          0.1465   \n1   F   0.630     0.490   0.145        1.1300          0.4580          0.2765   \n2   I   0.160     0.110   0.025        0.0210          0.0055          0.0030   \n3   M   0.595     0.475   0.150        0.9145          0.3755          0.2055   \n4   I   0.555     0.425   0.130        0.7820          0.3695          0.1600   \n\n   Shell_weight  Rings  original  \n0        0.2400     11         0  \n1        0.3200     11         0  \n2        0.0050      6         0  \n3        0.2500     10         0  \n4        0.1975      9         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Length</th>\n      <th>Diameter</th>\n      <th>Height</th>\n      <th>Whole_weight</th>\n      <th>Shucked_weight</th>\n      <th>Viscera_weight</th>\n      <th>Shell_weight</th>\n      <th>Rings</th>\n      <th>original</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F</td>\n      <td>0.550</td>\n      <td>0.430</td>\n      <td>0.150</td>\n      <td>0.7715</td>\n      <td>0.3285</td>\n      <td>0.1465</td>\n      <td>0.2400</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>F</td>\n      <td>0.630</td>\n      <td>0.490</td>\n      <td>0.145</td>\n      <td>1.1300</td>\n      <td>0.4580</td>\n      <td>0.2765</td>\n      <td>0.3200</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I</td>\n      <td>0.160</td>\n      <td>0.110</td>\n      <td>0.025</td>\n      <td>0.0210</td>\n      <td>0.0055</td>\n      <td>0.0030</td>\n      <td>0.0050</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>0.595</td>\n      <td>0.475</td>\n      <td>0.150</td>\n      <td>0.9145</td>\n      <td>0.3755</td>\n      <td>0.2055</td>\n      <td>0.2500</td>\n      <td>10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I</td>\n      <td>0.555</td>\n      <td>0.425</td>\n      <td>0.130</td>\n      <td>0.7820</td>\n      <td>0.3695</td>\n      <td>0.1600</td>\n      <td>0.1975</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2.1 Check Missing Values","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\n\ntable.field_names = ['Column Name', 'Data Type', 'Train Missing %', 'Test Missing %']\nfor column in train.columns:\n    data_type = str(train[column].dtype)\n    non_null_count_train= 100-train[column].count()/train.shape[0]*100\n    if column!=target:\n        non_null_count_test = 100-test[column].count()/test.shape[0]*100\n    else:\n        non_null_count_test=\"NA\"\n    table.add_row([column, data_type, non_null_count_train,non_null_count_test])\nprint(table)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:17:33.805179Z","iopub.execute_input":"2024-04-08T16:17:33.805687Z","iopub.status.idle":"2024-04-08T16:17:33.844242Z","shell.execute_reply.started":"2024-04-08T16:17:33.805652Z","shell.execute_reply":"2024-04-08T16:17:33.843163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">There are no missing values </font>","metadata":{}},{"cell_type":"markdown","source":"# 3. EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Target Analysis","metadata":{}},{"cell_type":"code","source":"class_0 = train[train['original'] == 0][target]\nclass_1 = train[train['original'] == 1][target]\n\nmean_0 = np.mean(class_0)\nmedian_0 = np.median(class_0)\nmean_1 = np.mean(class_1)\nmedian_1 = np.median(class_1)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.hist(class_0, bins=20, density=True, alpha=0.5, label='Original=0 Histogram')\nax.hist(class_1, bins=20, density=True, alpha=0.5, label='Original=1 Histogram')\n\nx_values_0 = np.linspace(class_0.min(), class_0.max(), len(class_0))\ndensity_values_0 = (1 / (np.sqrt(2 * np.pi) * np.std(class_0))) * np.exp(-0.5 * ((x_values_0 - mean_0) / np.std(class_0))**2)\nax.plot(x_values_0, density_values_0, color='red', label='Original=0 Density')\n\nx_values_1 = np.linspace(class_1.min(), class_1.max(), len(class_1))\ndensity_values_1 = (1 / (np.sqrt(2 * np.pi) * np.std(class_1))) * np.exp(-0.5 * ((x_values_1 - mean_1) / np.std(class_1))**2)\nax.plot(x_values_1, density_values_1, color='green', label='Original=1 Density')\n\nax.axvline(mean_0, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=0)')\nax.axvline(median_0, color='green', linestyle='dashed', linewidth=2, label='Median (Original=0)')\nax.axvline(mean_1, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=1)')\nax.axvline(median_1, color='red', linestyle='dashed', linewidth=2, label='Median (Original=1)')\n\nax.set_xlabel(target)\nax.set_ylabel('Frequency / Density')\nax.set_title('Histograms and Density Plots')\n\nx_min = min(min(class_0), min(class_1))\nx_max = max(max(class_0), max(class_1))\nax.set_xlim([x_min, x_max])\n\nax.legend(bbox_to_anchor=(1,1),fancybox=False,shadow=False, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:17:33.847065Z","iopub.execute_input":"2024-04-08T16:17:33.847436Z","iopub.status.idle":"2024-04-08T16:17:34.799616Z","shell.execute_reply.started":"2024-04-08T16:17:33.847401Z","shell.execute_reply":"2024-04-08T16:17:34.798237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_0 = np.log1p(train[train['original'] == 0][target])\nclass_1 = np.log1p(train[train['original'] == 1][target])\n\nmean_0 = np.mean(class_0)\nmedian_0 = np.median(class_0)\nmean_1 = np.mean(class_1)\nmedian_1 = np.median(class_1)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.hist(class_0, bins=20, density=True, alpha=0.5, label='Original=0 Histogram')\nax.hist(class_1, bins=20, density=True, alpha=0.5, label='Original=1 Histogram')\n\nx_values_0 = np.linspace(class_0.min(), class_0.max(), len(class_0))\ndensity_values_0 = (1 / (np.sqrt(2 * np.pi) * np.std(class_0))) * np.exp(-0.5 * ((x_values_0 - mean_0) / np.std(class_0))**2)\nax.plot(x_values_0, density_values_0, color='red', label='Original=0 Density')\n\nx_values_1 = np.linspace(class_1.min(), class_1.max(), len(class_1))\ndensity_values_1 = (1 / (np.sqrt(2 * np.pi) * np.std(class_1))) * np.exp(-0.5 * ((x_values_1 - mean_1) / np.std(class_1))**2)\nax.plot(x_values_1, density_values_1, color='green', label='Original=1 Density')\n\nax.axvline(mean_0, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=0)')\nax.axvline(median_0, color='green', linestyle='dashed', linewidth=2, label='Median (Original=0)')\nax.axvline(mean_1, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=1)')\nax.axvline(median_1, color='red', linestyle='dashed', linewidth=2, label='Median (Original=1)')\n\nax.set_xlabel(target)\nax.set_ylabel('Frequency / Density')\nax.set_title('Log Transformed Histograms and Density Plots')\n\nx_min = min(min(class_0), min(class_1))\nx_max = max(max(class_0), max(class_1))\nax.set_xlim([x_min, x_max])\n\nax.legend(bbox_to_anchor=(1,1),fancybox=False,shadow=False, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:17:34.801524Z","iopub.execute_input":"2024-04-08T16:17:34.802899Z","iopub.status.idle":"2024-04-08T16:17:35.556461Z","shell.execute_reply.started":"2024-04-08T16:17:34.802842Z","shell.execute_reply":"2024-04-08T16:17:35.554873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">There is less difference between normal Rings and log transformed Rings, however to deal with the evalutaion metric, I'm converting them to log transformed targets. I will test seperately with normal target to understand which works well</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Train & Test Data Distributions","metadata":{}},{"cell_type":"code","source":"cont_cols=[f for f in train.columns if train[f].dtype in [float,int] and train[f].nunique()>2 and f not in [target]]\n\n# Calculate the number of rows needed for the subplots\nnum_rows = (len(cont_cols) + 2) // 3\n\n# Create subplots for each continuous column\nfig, axs = plt.subplots(num_rows, 3, figsize=(15, num_rows*5))\n\n# Loop through each continuous column and plot the histograms\nfor i, col in enumerate(cont_cols):\n    # Determine the range of values to plot\n    max_val = max(train[col].max(), test[col].max(), original[col].max())\n    min_val = min(train[col].min(), test[col].min(), original[col].min())\n    range_val = max_val - min_val\n    \n    # Determine the bin size and number of bins\n    bin_size = range_val / 20\n    num_bins_train = round(range_val / bin_size)\n    num_bins_test = round(range_val / bin_size)\n    num_bins_original = round(range_val / bin_size)\n    \n    # Calculate the subplot position\n    row = i // 3\n    col_pos = i % 3\n    \n    # Plot the histograms\n    sns.histplot(train[col], ax=axs[row][col_pos], color='orange', kde=True, label='Train', bins=num_bins_train)\n    sns.histplot(test[col], ax=axs[row][col_pos], color='green', kde=True, label='Test', bins=num_bins_test)\n    sns.histplot(original[col], ax=axs[row][col_pos], color='blue', kde=True, label='Original', bins=num_bins_original)\n    axs[row][col_pos].set_title(col)\n    axs[row][col_pos].set_xlabel('Value')\n    axs[row][col_pos].set_ylabel('Frequency')\n    axs[row][col_pos].legend()\n\n# Remove any empty subplots\nif len(cont_cols) % 3 != 0:\n    for col_pos in range(len(cont_cols) % 3, 3):\n        axs[-1][col_pos].remove()\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:17:35.558604Z","iopub.execute_input":"2024-04-08T16:17:35.559433Z","iopub.status.idle":"2024-04-08T16:17:47.281504Z","shell.execute_reply.started":"2024-04-08T16:17:35.559388Z","shell.execute_reply":"2024-04-08T16:17:47.280558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INFERENCES**\n1. <font size=\"3\">Only Height follows a Normal Distribution</font>\n2. <font size=\"3\">Length Features & Weight Features have similar distribution across them. I suspect strong correlations between these categories. It is natural to have high shell weight & high visceral weight</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Sex & Numerical features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=original, vars=cont_cols+[target], hue='Sex')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:17:47.282838Z","iopub.execute_input":"2024-04-08T16:17:47.284051Z","iopub.status.idle":"2024-04-08T16:18:33.034534Z","shell.execute_reply.started":"2024-04-08T16:17:47.284013Z","shell.execute_reply":"2024-04-08T16:18:33.032255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INFERENCES**\n1. <font size=\"3\">We can see the growth of an Abalone in physical attributes when they mature </font>\n2. <font size=\"3\">There are few datapoints which are outliers because naturally difficult to have low weights, normal length, & 3 times taller than the population(Possibility of experimental errors/noise). </font> **That is not an Abalone!**\n3. <font size=\"3\">I think Sex is an important feature especially the immatured category</font>\n4. <font size=\"3\">Correlation across weights is observed and also between length-diameter. This is expected, crabs do look like a square from the top :)</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Rings vs Sex","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(16, 5))\nsns.violinplot(x='Sex', y=col, data=train)\nplt.title('Rings Distribution by Sex', fontsize=14)\nplt.xlabel('Sex', fontsize=12)\nplt.ylabel('Rings', fontsize=12)\nsns.despine()\nfig.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:18:33.036403Z","iopub.execute_input":"2024-04-08T16:18:33.036767Z","iopub.status.idle":"2024-04-08T16:18:34.519345Z","shell.execute_reply.started":"2024-04-08T16:18:33.036727Z","shell.execute_reply":"2024-04-08T16:18:34.517941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">There is a noticeable difference from Infant and Male/Female</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.5 Correlation Plot","metadata":{}},{"cell_type":"code","source":"features=[f for f in test.columns if f!='Sex']\ncorr = train[features].corr()\nplt.figure(figsize = (8, 6), dpi = 300)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, cmap = 'magma', annot = True, annot_kws = {'size' : 6})\nplt.title('Features Correlation Matrix\\n', fontsize = 15, weight = 'bold')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:18:34.521142Z","iopub.execute_input":"2024-04-08T16:18:34.521676Z","iopub.status.idle":"2024-04-08T16:18:35.641441Z","shell.execute_reply.started":"2024-04-08T16:18:34.521642Z","shell.execute_reply":"2024-04-08T16:18:35.639978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">We have high correlation as expected but they are important features</font>","metadata":{}},{"cell_type":"markdown","source":"# 4. FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"## Basic Functions","metadata":{}},{"cell_type":"code","source":"def min_max_scaler(train, test, column):\n    '''\n    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n    '''\n    sc=MinMaxScaler()\n    \n    max_val=max(train[column].max(),test[column].max())\n    min_val=min(train[column].min(),test[column].min())\n\n    train[column]=(train[column]-min_val)/(max_val-min_val)\n    test[column]=(test[column]-min_val)/(max_val-min_val)\n    \n    return train,test  \n\ndef rmse(y1,y2):\n    ''' RMSE Evaluator'''\n    return(np.sqrt(mean_squared_error(np.array(y1),np.array(y2))))\n\ndef nearest(y_predicted):\n    \n    y_original=y_unique\n    modified_prediction = np.zeros_like(y_predicted)\n\n    for i, y_pred in enumerate(y_predicted):\n        nearest_value = min(y_original, key=lambda x: abs(x - y_pred))\n        modified_prediction[i] = nearest_value\n\n    return modified_prediction\n\nglobal y_unique\ny_unique=train[target].unique()\ny_unique_log=np.log1p(train[target]).unique()\n\nxgb_params = {\n            'n_estimators': 500,\n            'max_depth': 6,\n            'learning_rate': 0.0116,\n            'colsample_bytree': 1,\n            'subsample': 0.6085,\n            'min_child_weight': 9,\n            'reg_lambda': 4.879e-07,\n            'max_bin': 431,\n            'n_jobs': -1,\n            'eval_metric': 'mae',\n            'objective': \"reg:absoluteerror\",\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': 42,\n        }\n\ndef fill_missing_numerical(train,test,target, max_iterations=10):\n    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n    train_temp=train.copy()\n    if target in train_temp.columns:\n        train_temp=train_temp.drop(columns=target)\n        \n    \n    df=pd.concat([train_temp,test],axis=\"rows\")\n    df=df.reset_index(drop=True)\n    features=[ f for f in df.columns if df[f].isna().sum()>0]\n    if len(features)>0:\n        # Step 1: Store the instances with missing values in each feature\n        missing_rows = store_missing_rows(df, features)\n\n        # Step 2: Initially fill all missing values with \"Missing\"\n#         for f in features:\n#             df[f]=df[f].fillna(df[f].median())\n\n        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n        dictionary = {feature: [] for feature in features}\n\n        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n            for feature in features:\n#                 print(feature)\n                # Skip features with no missing values\n                rows_miss = missing_rows[feature].index\n                replace_dict={}\n                rev_replace_dict={}\n                for col in  cat_features:\n                    df[col]=df[col].astype(str)\n                    int_cat=dict(zip(df[col].unique(),np.arange(0, df[col].nunique())))\n                    rev_int_cat=dict(zip(np.arange(0, df[col].nunique()), df[col].unique()))\n                    df[col]=df[col].replace(int_cat)\n                \n                    replace_dict[col]=int_cat\n                    rev_replace_dict[col]=rev_int_cat\n\n                missing_temp = df.loc[rows_miss].copy()\n                non_missing_temp = df.drop(index=rows_miss).copy()\n                y_pred_prev=missing_temp[feature]\n                missing_temp = missing_temp.drop(columns=[feature])\n\n\n                # Step 3: Use the remaining features to predict missing values using Random Forests\n                X_train = non_missing_temp.drop(columns=[feature])\n                y_train = non_missing_temp[[feature]]\n\n#                 model1 = CatBoostRegressor(**cb_params)\n#                 model1.fit(X_train, y_train,cat_features=cat_features, verbose=False)\n                \n                model2 = xgb.XGBRegressor(**xgb_params)\n                model2.fit(X_train, y_train, verbose=False) \n                \n                # Step 4: Predict missing values for the feature and update all N features\n                y_pred = np.array(model2.predict(missing_temp))\n                \n                df.loc[rows_miss, feature] = y_pred\n#                 error_minimize=rmse(y_pred,y_pred_prev) #mean_squared_error\n                error_minimize=np.sqrt(mean_squared_error(y_pred,y_pred_prev) )#mean_squared_error\n                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n                \n                for col in  cat_features:\n                    df[col]=df[col].replace(rev_int_cat)\n\n\n        for feature, values in dictionary.items():\n            values=np.array(values)/sum(values)\n            iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n            plt.plot(iterations, values, label=feature)  # plot the values\n            plt.xlabel('Iterations')\n            plt.ylabel('RMSE')\n            plt.title('Minimization of RMSE with iterations')\n            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.show()\n        train[features] = np.array(df.iloc[:train.shape[0]][features])\n        test[features] = np.array(df.iloc[train.shape[0]:][features])\n\n    return train,test","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:18:35.646618Z","iopub.execute_input":"2024-04-08T16:18:35.647088Z","iopub.status.idle":"2024-04-08T16:18:35.684883Z","shell.execute_reply.started":"2024-04-08T16:18:35.647054Z","shell.execute_reply":"2024-04-08T16:18:35.683238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.1 New Features","metadata":{}},{"cell_type":"markdown","source":"1. **Top Surface Area**:<font size=\"3\"> Length X Diameter</font>\n2. **Water Loss**: <font size=\"3\">During the experiment of data collection, it is possible to have some water loss after dissecting the crab to measure different weights</font>\n3. **Abalone Density**:<font size=\"3\">Measure of body density</font>\n4. **BMI:**<font size=\"3\">Body Mass index</font>\n5. **Measurement ratios:**<font size=\"3\">You can also calculate ratios like length/height, length/diameter. </font>\n6. **Incorrect Weights:**<font size=\"3\"> I have noticed that there are sub weight columns greater than the total weight of the Abalone. Part of the body cannot have more weight that the whole body</font>","metadata":{}},{"cell_type":"code","source":"def new_features(data):\n    df=data.copy()\n    \n    # Clean the weights by capping the over weights with total body weights\n    df['Shell_weight']=np.where(df['Shell_weight']>df['Whole_weight'],df['Whole_weight'],df['Shell_weight'])\n    df['Viscera_weight']=np.where(df['Viscera_weight']>df['Whole_weight'],df['Whole_weight'],df['Viscera_weight'])\n    df['Shucked_weight']=np.where(df['Shucked_weight']>df['Whole_weight'],df['Whole_weight'],df['Shucked_weight'])\n    \n    # Abalone Surface area\n    df[\"surface_area\"]=df[\"Length\"]*df[\"Diameter\"]\n    df['total_area']=2*(df[\"surface_area\"]+df[\"Height\"]*df[\"Diameter\"]+df[\"Length\"]*df[\"Height\"])\n    \n    # Abalone density approx\n    df['approx_density']=df['Whole_weight']/(df['surface_area']*df['Height']+1e-5)\n    \n    # Abalone BMI\n    df['bmi']=df['Whole_weight']/(df['Height']**2+1e-5)\n    \n    # Measurement derived\n    df[\"length_dia_ratio\"]=df['Length']/(df['Diameter']+1e-5)\n    df[\"length_height_ratio\"]=df['Length']/(df['Height']+1e-5)\n    df['shell_shuck_ratio']=df[\"Shell_weight\"]/(df[\"Shucked_weight\"]+1e-5)\n    df['shell_viscera_ratio']=df['Shell_weight']/(df['Viscera_weight']+1e-5)\n    \n    df['viscera_tot_ratio']=df['Viscera_weight']/(df['Whole_weight']  +1e-5)\n    df['shell_tot_ratio']=df['Shell_weight']/(df['Whole_weight']    +1e-5)\n    df['shuck_tot_ratio']=df['Shucked_weight']/(df['Whole_weight']   +1e-5)\n    df['shell_body_ratio']=df['Shell_weight']/(df['Shell_weight']+df['Whole_weight']+1e-5)\n    df['flesh_ratio']=df['Shucked_weight']/(df['Whole_weight']+df['Shucked_weight']+1e-5)\n    \n    df['inv_viscera_tot']= df['Whole_weight'] / (df['Viscera_weight']+1e-5)\n    df['inv_shell_tot']= df['Whole_weight'] /( df['Shell_weight']+1e-5)\n    df['inv_shuck_tot']= df['Whole_weight'] / (df['Shucked_weight']+1e-5)\n    \n    \n    # Water Loss during experiment\n    df[\"water_loss\"]=df[\"Whole_weight\"]-df[\"Shucked_weight\"]-df['Viscera_weight']-df['Shell_weight']\n    df[\"water_loss\"]=np.where(df[\"water_loss\"]<0,min(df[\"Shucked_weight\"].min(),df[\"Viscera_weight\"].min(),df[\"Shell_weight\"].min()),df[\"water_loss\"])\n    return df\n\ntrain=new_features(train)\ntest=new_features(test)\noriginal=new_features(original)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:18:35.688184Z","iopub.execute_input":"2024-04-08T16:18:35.68862Z","iopub.status.idle":"2024-04-08T16:18:35.799253Z","shell.execute_reply.started":"2024-04-08T16:18:35.688589Z","shell.execute_reply":"2024-04-08T16:18:35.797635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Numerical Transformations","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">We're going to see what transformation works better for each feature and select them, the idea is to compress the data. There could be situations where you will have to stretch the data. These are the methods applied:</font>\n\n1. **Log Transformation**: <font size=\"3\">This transformation involves taking the logarithm of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = log(x)\n\n2. **Square Root Transformation**: <font size=\"3\">This transformation involves taking the square root of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = sqrt(x)\n\n3. **Box-Cox Transformation**: <font size=\"3\">This transformation is a family of power transformations that includes the log and square root transformations as special cases. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = [(x^lambda) - 1] / lambda if lambda != 0\n                y = log(x) if lambda = 0\n\n4. **Yeo-Johnson Transformation**: <font size=\"3\">This transformation is similar to the Box-Cox transformation, but it can be applied to both positive and negative values. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = [(|x|^lambda) - 1] / lambda if x >= 0, lambda != 0\n                y = log(|x|) if x >= 0, lambda = 0\n                y = -[(|x|^lambda) - 1] / lambda if x < 0, lambda != 2\n                y = -log(|x|) if x < 0, lambda = 2\n\n5. **Power Transformation**: <font size=\"3\">This transformation involves raising each data point to a power. It is useful when the data is highly skewed and the variance increases with the mean. The power can be any value, and is often determined using statistical methods such as the Box-Cox or Yeo-Johnson transformations.</font>\n                y = [(x^lambda) - 1] / lambda if method = \"box-cox\" and lambda != 0\n                y = log(x) if method = \"box-cox\" and lambda = 0\n                y = [(x + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x >= 0, lambda != 0\n                y = log(x + 1) if method = \"yeo-johnson\" and x >= 0, lambda = 0\n                y = [-(|x| + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x < 0, lambda != 2\n                y = -log(|x| + 1) if method = \"yeo-johnson\" and x < 0, lambda = 2","metadata":{}},{"cell_type":"code","source":"cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique()>10000]\n\nsc=MinMaxScaler()\n\nglobal unimportant_features\nglobal overall_best_score\nglobal overall_best_col\nunimportant_features=[]\noverall_best_score=1e5\noverall_best_col='none'\n\n# for col in cont_cols:\n#      train, test=min_max_scaler(train, test, col)\n\ndef transformer(train, test,cont_cols, target):\n    '''\n    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n    '''\n    global unimportant_features\n    global overall_best_score\n    global overall_best_col\n    train_copy = train.copy()\n    test_copy = test.copy()\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Original RMSLE', 'Transformation', 'Tranformed RMSLE']\n\n    for col in cont_cols:\n        train_copy, test_copy=min_max_scaler(train_copy, test_copy, col)\n        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n            if c in train_copy.columns:\n                train_copy = train_copy.drop(columns=[c])\n        \n        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n        \n        # Square Root Transformation\n        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n        \n        # Box-Cox transformation\n        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n        epsilon = 1e-5\n        transformer = PowerTransformer(method='box-cox')\n#         scaled_data = transformer.fit_transform(combined_data + epsilon)\n\n#         train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n#         test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n        train_copy[\"bx_cx_\" + col] = transformer.fit_transform(train_copy[[col]]+epsilon)\n        test_copy[\"bx_cx_\" + col] = transformer.transform(test_copy[[col]]+epsilon)\n        # Yeo-Johnson transformation\n        transformer = PowerTransformer(method='yeo-johnson')\n        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 0.25\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 2\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Log to power transformation\n        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n        \n        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col,  \"pow_\"+col , \"pow2_\"+col,\"log_sqrt\"+col]\n        \n        train_copy,test_copy = fill_missing_numerical(train_copy,test_copy,target,3)\n#         train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n#         test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n        \n        pca = TruncatedSVD(n_components=1)\n        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n        x_pca_test = pca.transform(test_copy[temp_cols])\n        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n        temp_cols.append(col+\"_pca_comb\")\n        \n        test_copy = test_copy.reset_index(drop=True)\n        \n        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n        \n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        \n        rmse_scores = []\n        \n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].values\n            \n            rmses = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n                model=LinearRegression()\n                model.fit(X_train,np.log1p(y_train))\n                y_pred=nearest(np.expm1(model.predict(x_val)))\n                rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n            rmse_scores.append((f,np.mean(rmses)))\n            \n            if overall_best_score > np.mean(rmses):\n                overall_best_score = np.mean(rmses)\n                overall_best_col = f\n\n            if f == col:\n                orig_rmse = np.mean(rmses)\n                \n        best_col, best_rmse = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n        cols_to_drop = [f for f in temp_cols if f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        \n        if cols_to_drop:\n            unimportant_features = unimportant_features+cols_to_drop\n        table.add_row([col,orig_rmse,best_col ,best_rmse])\n    print(table)   \n    print(\"overall best CV RMSLE score: \",overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= transformer(train, test,cont_cols, target)\ntrain, test=fill_missing_numerical(train,test,target, max_iterations=3)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:18:35.802369Z","iopub.execute_input":"2024-04-08T16:18:35.803268Z","iopub.status.idle":"2024-04-08T16:23:03.235312Z","shell.execute_reply.started":"2024-04-08T16:18:35.803203Z","shell.execute_reply":"2024-04-08T16:23:03.2339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Numerical Clustering","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = ['Cluster WOE Feature', 'RMSLE (CV-TRAIN)']\nfor col in cont_cols:\n    sub_set=[f for f in unimportant_features if col in f]\n#     print(sub_set)\n    temp_train=train[sub_set]\n    temp_test=test[sub_set]\n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    model = KMeans()\n\n    # print(ideal_clusters)\n    kmeans = KMeans(n_clusters=5)\n    kmeans.fit(np.array(temp_train))\n    labels_train = kmeans.labels_\n\n    train[col+\"_unimp_cluster_WOE\"] = labels_train\n    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n    \n    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    X=train[[col+\"_unimp_cluster_WOE\"]].values\n    y=train[target].values\n\n    best_rmse=[]\n    for train_idx, val_idx in kf.split(X,y):\n        X_train,y_train=X[train_idx],y[train_idx]\n        x_val,y_val=X[val_idx],y[val_idx]\n        model=LinearRegression()\n        model.fit(X_train,np.log1p(y_train))\n        y_pred=nearest(np.expm1(model.predict(x_val)))\n        best_rmse.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n        \n    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(best_rmse)])\n    if overall_best_score<np.mean(best_rmse):\n            overall_best_score=np.mean(best_rmse)\n            overall_best_col=col+\"_unimp_cluster_WOE\"\n    \nprint(table)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:23:03.23731Z","iopub.execute_input":"2024-04-08T16:23:03.23766Z","iopub.status.idle":"2024-04-08T16:24:15.454716Z","shell.execute_reply.started":"2024-04-08T16:23:03.237631Z","shell.execute_reply":"2024-04-08T16:24:15.452126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Encoding","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">For each categorical variable, perform the following encoding techniques:</font>\n1. <font size=\"3\">**Count/Frequency Encoding**</font>: <font size=\"3\">Count the number of occurrences of each category and replace the category with its log count.</font>\n2. <font size=\"3\">**Count Labeling**</font>: <font size=\"3\">Assign a label to each category based on its count, with higher counts receiving higher labels.</font>\n3. <font size=\"3\"> **Target-Guided Mean Encoding**</font>: <font size=\"3\">Rank the categories based on the mean of target column across each category</font>\n\n<font size=\"3\"> Please note that the features a particular encoding technique is not selected only if it has superior technique and the correlation with that is high</font>","metadata":{}},{"cell_type":"code","source":"cat_cols = [f for f in test.columns if (train[f].dtype != 'O' and train[f].nunique()<2000 and train[f].nunique()>2 and \"WOE\" not in f) or (train[f].dtype == 'O') ]\nprint(train[cat_cols].nunique())","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:55.990306Z","iopub.execute_input":"2024-04-08T16:24:55.991745Z","iopub.status.idle":"2024-04-08T16:24:56.548262Z","shell.execute_reply.started":"2024-04-08T16:24:55.991703Z","shell.execute_reply":"2024-04-08T16:24:56.546877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stabilize Features","metadata":{}},{"cell_type":"code","source":"def nearest_val(target):\n    return min(common, key=lambda x: abs(x - target))\n\ncat_cols_updated=['Sex']\nfor col in cat_cols:\n    if train[col].dtype!=\"O\":\n        train[f\"{col}_cat\"]=train[col]\n        test[f\"{col}_cat\"]=test[col]\n        cat_cols_updated.append(f\"{col}_cat\")\n        uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n        if uncommon:\n            common=list(set(test[col].unique())& set(train[col].unique()))\n            train[f\"{col}_cat\"]=train[col].apply(nearest_val)\n            test[f\"{col}_cat\"]=test[col].apply(nearest_val)\nprint(train[cat_cols_updated].nunique())","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:25:03.310222Z","iopub.execute_input":"2024-04-08T16:25:03.310632Z","iopub.status.idle":"2024-04-08T16:29:04.837413Z","shell.execute_reply.started":"2024-04-08T16:25:03.310602Z","shell.execute_reply":"2024-04-08T16:29:04.835882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global overall_best_score\n# overall_best_score = 0\ndef OHE(train_df,test_df,cols,target):\n    '''\n    Function for one hot encoding, it first combines the data so that no category is missed and\n    the category with least frequency can be dropped because of redundancy\n    '''\n    combined = pd.concat([train_df, test_df], axis=0)\n    for col in cols:\n        one_hot = pd.get_dummies(combined[col])\n        counts = combined[col].value_counts()\n        min_count_category = counts.idxmin()\n        one_hot = one_hot.drop(min_count_category, axis=1)\n        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n        combined = pd.concat([combined, one_hot], axis=\"columns\")\n        combined = combined.loc[:, ~combined.columns.duplicated()]\n    \n    # split back to train and test dataframes\n    train_ohe = combined[:len(train_df)]\n    test_ohe = combined[len(train_df):]\n    test_ohe.reset_index(inplace=True,drop=True)\n    test_ohe.drop(columns=[target],inplace=True)\n    return train_ohe, test_ohe\n\ndef high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n    '''\n    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n    '''\n    train_copy=train.copy()\n    test_copy=test.copy()\n    ohe_cols=[]\n    for col in extra_cols:\n        dict1=train_copy[col].value_counts().to_dict()\n        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n        rare_keys=list([*ordered.keys()][n_limit:])\n#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n        \n        train_copy[col]=train_copy[col].replace(rare_key_map)\n        test_copy[col]=test_copy[col].replace(rare_key_map)\n    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n    train_copy=train_copy.drop(columns=drop_cols)\n    test_copy=test_copy.drop(columns=drop_cols)\n    \n    return train_copy, test_copy\n\ndef cat_encoding(train, test,cat_cols_updated, target):\n    global overall_best_score\n    global overall_best_col\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Encoded Features', 'RMSLE Score']\n    train_copy=train.copy()\n    test_copy=test.copy()\n    train_dum = train.copy()\n    for feature in cat_cols_updated:\n#         print(feature)\n#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n\n        dic = train[feature].value_counts().to_dict()\n        train_copy[feature + \"_count\"] =train[feature].map(dic)\n        test_copy[feature + \"_count\"] = test[feature].map(dic)\n\n        dic2=train[feature].value_counts().to_dict()\n#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n        list1=np.arange(len(dic2.values())) # Higher rank for low count\n        dic3=dict(zip(list(dic2.keys()),list1))\n        \n        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n\n        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#,feature + \"_target\"\n\n        train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n        test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n        \n        if train_copy[feature].nunique()<=15:\n            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n            \n        else:\n            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=15)\n            \n        train_copy=train_copy.drop(columns=[feature])\n        test_copy=test_copy.drop(columns=[feature])\n\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n        rmse_scores = []\n\n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].astype(int).values\n\n            rmses = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n                model=LinearRegression()\n                model.fit(X_train,np.log1p(y_train))\n                y_pred=nearest(np.expm1(model.predict(x_val)))\n                rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n            rmse_scores.append((f,np.mean(rmses)))\n            if overall_best_score > np.mean(rmses):\n                overall_best_score = np.mean(rmses)\n                overall_best_col = f\n        best_col, best_auc = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n\n        corr = train_copy[temp_cols].corr(method='pearson')\n        corr_with_best_col = corr[best_col]\n        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        if cols_to_drop:\n            train_copy = train_copy.drop(columns=cols_to_drop)\n            test_copy = test_copy.drop(columns=cols_to_drop)\n\n        table.add_row([feature, best_col, best_auc])\n        print(feature)\n    print(table)\n    print(\"overall best CV score: \", overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= cat_encoding(train, test,cat_cols_updated, target)\ntrain, test=fill_missing_numerical(train,test,target, max_iterations=3)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:29:04.839904Z","iopub.execute_input":"2024-04-08T16:29:04.840278Z","iopub.status.idle":"2024-04-08T16:31:31.76446Z","shell.execute_reply.started":"2024-04-08T16:29:04.840248Z","shell.execute_reply":"2024-04-08T16:31:31.763254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5 Feature Elimination","metadata":{}},{"cell_type":"code","source":"first_drop=[ f for f in unimportant_features if f in train.columns]\ntrain=train.drop(columns=first_drop)\ntest=test.drop(columns=first_drop)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:31:31.766249Z","iopub.execute_input":"2024-04-08T16:31:31.766847Z","iopub.status.idle":"2024-04-08T16:31:31.814299Z","shell.execute_reply.started":"2024-04-08T16:31:31.766812Z","shell.execute_reply":"2024-04-08T16:31:31.813285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Steps to Eliminate Correlated  Features**:\n1. <font size=\"3\">Group features based on their parent feature. For example, all features derived from weight come under one set</font>\n2. <font size=\"3\">Apply PCA on the set, Cluster-Target Encoding on the set</font>\n3. <font size=\"3\">See the performance of each feature on a cross-validated single feature-target model</font>\n4. <font size=\"3\">Select the feature with highest CV-MAE</font>","metadata":{}},{"cell_type":"code","source":"final_drop_list=[]\n\ntable = PrettyTable()\ntable.field_names = ['Original', 'Final Transformation', \"RMSLE(CV)- Regression\"]\ndt_params={'criterion': 'absolute_error'}\nthreshold=0.85\n# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\nbest_cols=[]\n\nfor col in cont_cols:\n    sub_set=[f for f in train.columns if col in f and train[f].nunique()>100]\n    print(sub_set)\n    if len(sub_set)>2:\n        correlated_features = []\n\n        for i, feature in enumerate(sub_set):\n            # Check correlation with all remaining features\n            for j in range(i+1, len(sub_set)):\n                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n                # If correlation is greater than threshold, add to list of highly correlated features\n                if correlation > threshold:\n                    correlated_features.append(sub_set[j])\n\n        # Remove duplicate features from the list\n        correlated_features = list(set(correlated_features))\n        print(correlated_features)\n        if len(correlated_features)>=2:\n\n            temp_train=train[correlated_features]\n            temp_test=test[correlated_features]\n            #Scale before applying PCA\n            sc=StandardScaler()\n            temp_train=sc.fit_transform(temp_train)\n            temp_test=sc.transform(temp_test)\n\n            # Initiate PCA\n            pca=TruncatedSVD(n_components=1)\n            x_pca_train=pca.fit_transform(temp_train)\n            x_pca_test=pca.transform(temp_test)\n            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n            train=pd.concat([train,x_pca_train],axis='columns')\n            test=pd.concat([test,x_pca_test],axis='columns')\n\n            # Clustering\n            model = KMeans()\n            kmeans = KMeans(n_clusters=28)\n            kmeans.fit(np.array(temp_train))\n            labels_train = kmeans.labels_\n\n            train[col+'_final_cluster'] = labels_train\n            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n\n            cat_labels=cat_labels=train.groupby([col+\"_final_cluster\"])[target].mean()\n            cat_labels2=cat_labels.to_dict()\n            train[col+\"_final_cluster\"]=train[col+\"_final_cluster\"].map(cat_labels2)\n            test[col+\"_final_cluster\"]=test[col+\"_final_cluster\"].map(cat_labels2)\n\n            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n\n            # See which transformation along with the original is giving you the best univariate fit with target\n            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n\n            rmse_scores = []\n\n            for f in temp_cols:\n                X = train_copy[[f]].values\n                y = train_copy[target].astype(int).values\n\n                rmses = []\n                for train_idx, val_idx in kf.split(X, y):\n                    X_train, y_train = X[train_idx], y[train_idx]\n                    x_val, y_val = X[val_idx], y[val_idx]\n                    model=LinearRegression()\n                    model.fit(X_train,np.log1p(y_train))\n                    y_pred=nearest(np.expm1(model.predict(x_val)))\n                    rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n                    \n                if f not in best_cols:\n                    rmse_scores.append((f,np.mean(rmses)))\n            best_col, best_rmse=sorted(rmse_scores, key=lambda x:x[1], reverse=False)[0]\n            best_cols.append(best_col)\n\n            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n            if cols_to_drop:\n                final_drop_list=final_drop_list+cols_to_drop\n            table.add_row([col,best_col ,best_acc])\n\nprint(table)      ","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:31:31.816896Z","iopub.execute_input":"2024-04-08T16:31:31.817491Z","iopub.status.idle":"2024-04-08T16:31:31.906896Z","shell.execute_reply.started":"2024-04-08T16:31:31.817459Z","shell.execute_reply":"2024-04-08T16:31:31.905847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6 Feature Selection","metadata":{}},{"cell_type":"code","source":"final_features=[f for f in train.columns if f not in [target]]\nfinal_features=[*set(final_features)]\n\nsc=StandardScaler()\n\ntrain_scaled=train.copy()\ntest_scaled=test.copy()\ntrain_scaled[final_features]=sc.fit_transform(train[final_features])\ntest_scaled[final_features]=sc.transform(test[final_features])\nlen(final_features)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:31:31.908402Z","iopub.execute_input":"2024-04-08T16:31:31.909Z","iopub.status.idle":"2024-04-08T16:31:34.728873Z","shell.execute_reply.started":"2024-04-08T16:31:31.908967Z","shell.execute_reply":"2024-04-08T16:31:34.727802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_processor(train, test):\n    cols=train.drop(columns=[target]).columns\n    train_cop=train.copy()\n    test_cop=test.copy()\n    drop_cols=[]\n    for i, feature in enumerate(cols):\n        for j in range(i+1, len(cols)):\n            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n                if cols[j] not in drop_cols:\n                    drop_cols.append(cols[j])\n    print(drop_cols)\n    train_cop.drop(columns=drop_cols,inplace=True)\n    test_cop.drop(columns=drop_cols,inplace=True)\n    \n    return train_cop, test_cop\n                    \ntrain_cop, test_cop=   post_processor(train_scaled, test_scaled)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:31:34.730406Z","iopub.execute_input":"2024-04-08T16:31:34.731014Z","iopub.status.idle":"2024-04-08T16:36:20.144709Z","shell.execute_reply.started":"2024-04-08T16:31:34.73098Z","shell.execute_reply":"2024-04-08T16:36:20.143053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"data=train_cop[train_cop[target].isin([7,8,9,10,11])]\ndata_=train_cop[~train_cop[target].isin([7,8,9,10,11])]\n\nfrom sklearn.ensemble import IsolationForest\n\ndef isolation_forest(data):\n    data_pure=data.copy()\n    \n#     model = IsolationForest(contamination=0.010650427383702971, random_state=0)\n    model = IsolationForest(contamination=0.01, random_state=0)\n    \n    drop_cols=[f for f in data_pure.columns if f in target]\n    model.fit(data_pure.drop(columns=drop_cols))\n\n    # Predict the anomaly scores for each data point\n    anomalies = model.predict(data_pure.drop(columns=drop_cols))\n\n    outliers = anomalies == -1\n\n    # Combine the outlier information with the original data and labels\n    data_pure['outlier_ISF'] = outliers\n\n    # Print the identified outliers\n    identified_outliers = data_pure[data_pure['outlier_ISF']]\n    print(f\"Number of detected Potential outliers: {identified_outliers.shape[0]}\")\n    \n    data_clean=data[~data_pure['outlier_ISF']]\n    print(data_clean.shape)\n    \n    return data_clean\n\n\ndata_clean=isolation_forest(data).reset_index(drop=True)\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# params={'n_neighbors': 13, 'contamination': 0.01019797151100069}\nparams={'n_neighbors': 13, 'contamination': 0.01}\n\n\ndef lof(data):\n    data_pure=data.copy()\n    drop_cols=[f for f in data_pure.columns if f in target]\n\n    features = data_pure.drop(columns=drop_cols)\n    lof = LocalOutlierFactor(**params)  # Adjust contamination based on your data\n    anomalies = lof.fit_predict(features)  # Negative scores are outliers\n\n    outliers = anomalies == -1\n\n    # Combine the outlier information with the original data and labels\n    data_pure['outliers_LOF'] = outliers\n\n    # Print the identified outliers\n    identified_outliers = data_pure[data_pure['outliers_LOF']]\n    print(f\"Number of detected Potential outliers: {identified_outliers.shape[0]}\")\n    \n    data_clean=data[~data_pure['outliers_LOF']]\n    print(data_clean.shape)\n    \n    return data_clean\n\ndata_clean=lof(data_clean).reset_index(drop=True)\n\ndef pca_anamolies(data):\n    data_pure=data.copy()\n    drop_cols=[f for f in data_pure.columns if f in target]\n\n    features = data_pure.drop(columns=drop_cols)\n\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    pca = PCA(n_components=2)  # Choose the number of components for visualization\n    principal_components = pca.fit_transform(scaled_features)\n\n    # Calculate the reconstruction error (MSE) for each data point\n    reconstruction_errors = ((scaled_features - pca.inverse_transform(principal_components)) ** 2).mean(axis=1)\n\n    # Set a threshold for anomaly detection\n    threshold = 3.5 # Adjust the threshold based on your data and desired sensitivity\n\n    # Identify potential outliers\n    potential_outliers = [index for index, error in enumerate(reconstruction_errors) if error > threshold]\n\n    # Create a new column 'outliers' in the DataFrame\n    data_pure['outliers_PCA'] = False\n    data_pure.loc[potential_outliers, 'outliers_PCA'] = True\n\n    print(f\"Number of detected Potential outliers: {len(potential_outliers)}\")\n    \n    data_clean=data[~data_pure['outliers_PCA']]\n    print(data_clean.shape)\n\n    # Plot the data with potential outliers highlighted\n    plt.scatter(principal_components[:, 0], principal_components[:, 1], c='green', label='Normal Data')\n    plt.scatter(principal_components[potential_outliers, 0], principal_components[potential_outliers, 1], c='red', label='Potential Outliers')\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend()\n    plt.title('PCA with Potential Outliers')\n    plt.show()\n    return data_clean\n\ndata_clean=pca_anamolies( data_clean).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:36:20.146704Z","iopub.execute_input":"2024-04-08T16:36:20.147144Z","iopub.status.idle":"2024-04-08T16:37:34.595038Z","shell.execute_reply.started":"2024-04-08T16:36:20.14711Z","shell.execute_reply":"2024-04-08T16:37:34.593382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_cop=pd.concat([data_clean,data_],axis=\"rows\").reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:37:34.600072Z","iopub.execute_input":"2024-04-08T16:37:34.60054Z","iopub.status.idle":"2024-04-08T16:37:34.606301Z","shell.execute_reply.started":"2024-04-08T16:37:34.600506Z","shell.execute_reply":"2024-04-08T16:37:34.604711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_cop.drop(columns=[target])\ny_train = train_cop[target]\n\nX_test = test_cop.copy()\n\nprint(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:37:34.608374Z","iopub.execute_input":"2024-04-08T16:37:34.60891Z","iopub.status.idle":"2024-04-08T16:37:34.720255Z","shell.execute_reply.started":"2024-04-08T16:37:34.608866Z","shell.execute_reply":"2024-04-08T16:37:34.718882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_most_important_features(X_train, y_train, n,model_input):\n    \n    lgb_params = {\n            'n_estimators': 100,\n            'max_depth': 6,\n            \"num_leaves\": 16,\n            'learning_rate': 0.05,\n            'subsample': 0.7,\n            'colsample_bytree': 0.8,\n            #'reg_alpha': 0.25,\n            'reg_lambda': 5e-07,\n            'objective': 'regression_l2',\n            'metric': 'mean_absolute_error',\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'verbose':-1\n        }\n    cb_params = {\n            'iterations': 300,\n            'depth': 6,\n            'learning_rate': 0.01,\n            'l2_leaf_reg': 0.5,\n            'random_strength': 0.2,\n            'max_bin': 150,\n            'od_wait': 80,\n            'one_hot_max_size': 70,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'IncToDec',\n            'eval_metric': 'MSLE',\n            'loss_function': 'RMSE',\n            'random_state': 42,\n             'verbose':False\n        }\n     \n    xgb_params = {\n            'n_estimators': 500,\n            'max_depth': 6,\n            'learning_rate': 0.0116,\n            'colsample_bytree': 1,\n            'min_child_weight': 9,\n            'n_jobs': -1,\n            'eval_metric': 'rmsle',\n            'objective': \"reg:squarederror\",\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': 42,\n        }\n    if 'xgb' in model_input:\n        model = xgb.XGBRegressor(**xgb_params)\n    elif 'cat' in model_input:\n        model=CatBoostRegressor(**cb_params)\n    else:\n        model=lgb.LGBMRegressor(**lgb_params)\n    \n    \n\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    rmse_scores = []\n\n    for train_idx, val_idx in kfold.split(X_train,y_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        if 'lgb' in model_input:\n            model.fit(X_train_fold, np.log1p(y_train_fold))\n        else:\n            model.fit(X_train_fold, np.log1p(y_train_fold),verbose=False)\n\n        y_pred = model.predict(X_val_fold)\n\n        rmses = np.sqrt(mean_squared_error(np.log1p(y_val_fold),y_pred))\n        rmse_scores.append(rmses)\n\n    avg_rmse = np.mean(rmse_scores)\n\n    feature_importances = model.feature_importances_\n\n    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(feature_importances)]\n\n    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n\n    top_n_features = [feature[0] for feature in sorted_features[:n]]\n    print(avg_rmse)\n    return top_n_features\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:37:34.725651Z","iopub.execute_input":"2024-04-08T16:37:34.726088Z","iopub.status.idle":"2024-04-08T16:37:34.747102Z","shell.execute_reply.started":"2024-04-08T16:37:34.726057Z","shell.execute_reply":"2024-04-08T16:37:34.745542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,75, 'cat')\nn_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,75, 'xgb')\nn_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train, 75, 'lgbm')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:03:30.59556Z","iopub.execute_input":"2024-04-08T17:03:30.596016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features=[*set(n_imp_features_lgbm+n_imp_features_cat)]#\nprint(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")\n\nX_train=X_train[n_imp_features]\nX_test=X_test[n_imp_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Weights","metadata":{}},{"cell_type":"code","source":"classes = np.unique(y_train)  # Get unique class labels\nclass_to_index = {cls: idx for idx, cls in enumerate(classes)}\ny_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n\nclass_counts = np.bincount(y_train_numeric)\n\ntotal_samples = len(y_train_numeric)\n\nclass_weights = total_samples / (len(classes) * class_counts)\n\nclass_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nprint(\"Class counts:\", class_counts)\nprint(\"Total samples:\", total_samples)\nprint(\"Class weights:\", class_weights)\nprint(\"Class weights dictionary:\", class_weights_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. MODELING","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Along with Regression Models, I'm also testing Multiclass, only because we have enough data. So, I should work decently or atleast model some untapped regions</font>","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Models","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout\n\nsgd=tensorflow.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\nrms = tensorflow.keras.optimizers.RMSprop()\nnadam=tensorflow.keras.optimizers.Nadam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n)\nlrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)\n\nfrom tensorflow.keras import backend as K\n\ndef root_mean_squared_log_error(y_true, y_pred):\n    '''\n    Compute RMSLE between actuals & predictions\n    '''\n    return K.sqrt(K.mean(K.square(K.log(abs(y_pred+1)) - K.log(abs(y_true+1)))))\n    \ndef root_mean_squared_error(y_true, y_pred):\n    '''\n    Compute RMSE between actuals & predictions\n    '''\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \nann = Sequential()\nann.add(Dense(64, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\nann.add(Dropout(0.1))\nann.add(Dense(16,  kernel_initializer='he_uniform', activation='relu'))\nann.add(Dropout(0.1))\nann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\nann.add(Dropout(0.1))\n\nann.add(Dense(1,  kernel_initializer='he_uniform'))\nann.compile(loss=root_mean_squared_log_error, optimizer=\"Adamax\",metrics=['mse'])\n\n    \nann2 = Sequential()\nann2.add(Dense(12, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\nann2.add(Dropout(0.1))\nann2.add(Dense(8,  kernel_initializer='he_uniform', activation='relu'))\nann2.add(Dropout(0.1))\n# ann2.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\n# ann2.add(Dropout(0.1))\n\nann2.add(Dense(1,  kernel_initializer='he_uniform'))\nann2.compile(loss=root_mean_squared_log_error, optimizer=\"Adam\",metrics=['mse'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size\n        self.kfold = kfold\n        self.n_splits = n_splits\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            for random_state in random_state_list:\n                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=random_state)\n                yield X_train, X_val, y_train, y_val\n\nclass Regressor:\n    def __init__(self, n_estimators=100, device=\"gpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.reg_models = self._define_reg_model()\n        self.len_models = len(self.reg_models)\n        \n    def _define_reg_model(self):\n        \n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 6,\n            'learning_rate': 0.0116,\n            'colsample_bytree': 1,\n            'subsample': 0.6085,\n            'min_child_weight': 9,\n            'reg_lambda': 4.879e-07,\n            'max_bin': 431,\n            #'booster': 'dart',\n            'n_jobs': -1,\n            'eval_metric': 'rmsle',\n            'objective': \"reg:squaredlogerror\",\n            #'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': self.random_state,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n        \n        xgb_params1=xgb_params.copy()\n        xgb_params1['subsample']=0.7\n        xgb_params1['max_depth']=10\n        xgb_params1['learning_rate']=0.002\n        xgb_params1['colsample_bytree']=0.6\n\n        xgb_params2=xgb_params.copy()\n        xgb_params2['subsample']=0.5\n        xgb_params2['max_depth']=8\n        xgb_params2['learning_rate']=0.047\n        xgb_params2['colsample_bytree']=0.9\n        xgb_params2['tree_method']='approx'\n     \n        lgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 6,\n            \"num_leaves\": 16,\n            'learning_rate': 0.002,\n            'subsample': 0.7,\n            'colsample_bytree': 0.8,\n            #'reg_alpha': 0.25,\n            'reg_lambda': 5e-07,\n            'objective': 'regression_l2',\n            'metric': 'mean_squared_error',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n            'verbose':-1\n        }\n        lgb_params1=lgb_params.copy()\n        lgb_params1['subsample']=0.9\n        lgb_params1['reg_lambda']=0.8994221730208598\n        lgb_params1['reg_alpha']=0.6236579699090548\n        lgb_params1['max_depth']=6\n        lgb_params1['learning_rate']=0.01\n        lgb_params1['colsample_bytree']=0.5\n\n        lgb_params2=lgb_params.copy()\n        lgb_params2['subsample']=0.1\n        lgb_params2['reg_lambda']=0.5940716788024517\n        lgb_params2['reg_alpha']=0.4300477974434703\n        lgb_params2['max_depth']=8\n        lgb_params2['learning_rate']=0.019000000000000003\n        lgb_params2['colsample_bytree']=0.8\n        lgb_params3 = {\n            'n_estimators': self.n_estimators,\n            'num_leaves': 45,\n            'max_depth': 5,\n            'learning_rate': 0.0684383311038932,\n            'subsample': 0.5758412171285148,\n            'colsample_bytree': 0.8599714680300794,\n            'reg_lambda': 1.597717830931487e-08,\n            'objective': 'regression_l2',\n            'metric': 'mean_squared_error',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n            'verbosity': 0,\n            'force_col_wise': True,\n            'verbose':-1\n        }\n        lgb_params4=lgb_params.copy()\n        lgb_params4['subsample']=0.3\n        lgb_params4['reg_lambda']=0.5488355125638069\n        lgb_params4['reg_alpha']=0.23414681424407247\n        lgb_params4['max_depth']=7\n        lgb_params4['learning_rate']=0.019000000000000003\n        lgb_params4['colsample_bytree']=0.5\n        lgb_params4['verbose']=-1\n        \n        lgb_class = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 8,\n            'learning_rate': 0.02,\n            'subsample': 0.20,\n            'colsample_bytree': 0.56,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-08,\n            'objective': 'multi_logloss',\n            'metric': 'multi_logloss',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n            'class_weight':class_weights_dict,\n        }\n        \n        cb_params = {\n            'iterations': self.n_estimators,\n            'depth': 6,\n            'learning_rate': 0.002,\n            'l2_leaf_reg': 0.5,\n            'random_strength': 0.2,\n            'max_bin': 150,\n            'od_wait': 80,\n            'one_hot_max_size': 70,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'IncToDec',\n            'eval_metric': 'MSLE',\n            'loss_function': 'RMSE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        cb_sym_params = cb_params.copy()\n        cb_sym_params['grow_policy'] = 'SymmetricTree'\n        cb_loss_params = cb_params.copy()\n        cb_loss_params['grow_policy'] = 'Lossguide'\n    \n        cb_params1 = {\n            'iterations': self.n_estimators,\n            'depth': 15,\n            'learning_rate': 0.05,\n            'l2_leaf_reg': 0.1,\n            'random_strength': 0.2,\n            'max_bin': 150,\n            'od_wait': 50,\n            'one_hot_max_size': 70,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bernoulli',\n            'od_type': 'Iter',\n            'eval_metric': 'MSLE',\n            'loss_function': 'RMSE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        cb_params2= {\n            'n_estimators': self.n_estimators,\n            'depth': 10,\n            'learning_rate': 0.08827842054729117,\n            'l2_leaf_reg': 4.8351074756668864e-05,\n            'random_strength': 0.21306687539993183,\n            'max_bin': 483,\n            'od_wait': 97,\n            'grow_policy': 'Lossguide',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MSLE',\n            'loss_function': 'RMSE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n            'silent': True\n        }\n        cat_class1={\n             'iterations': 500,\n            'depth': 6,\n            'learning_rate': 0.002,\n            'l2_leaf_reg': 0.7,\n            'random_strength': 0.2,\n            'max_bin': 200,\n            'od_wait': 65,\n            'one_hot_max_size': 70,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MultiClass',\n            'loss_function': 'MultiClass',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n}\n        cat_class2={\n            'iterations': self.n_estimators,\n            'random_strength': 0.1, \n            'one_hot_max_size': 70, \n            'max_bin': 100, \n            'learning_rate': 0.003, \n            'l2_leaf_reg': 0.3, \n            'grow_policy': 'Depthwise', \n            'depth': 9, \n            'max_bin': 200,\n            'od_wait': 65,\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MultiClass',\n            'loss_function': 'MultiClass',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        }\n        dt_params= {'min_samples_split': 8, 'min_samples_leaf': 4, 'max_depth': 16, 'criterion': 'squared_error'}\n        knn_params= {'weights': 'uniform', 'p': 1, 'n_neighbors': 12, 'leaf_size': 20, 'algorithm': 'kd_tree'}\n\n        reg_models = {\n            'xgb_reg': xgb.XGBRegressor(**xgb_params),\n           'xgb_reg1': xgb.XGBRegressor(**xgb_params1),\n           'xgb_reg2': xgb.XGBRegressor(**xgb_params2),\n            'lgb_reg': lgb.LGBMRegressor(**lgb_params),\n#             'lgb2_reg': lgb.LGBMRegressor(**lgb_params1),\n#            'lgb3_reg': lgb.LGBMRegressor(**lgb_params2),\n#             'lgb4_reg': lgb.LGBMRegressor(**lgb_params3),\n#           'lgb5_reg': lgb.LGBMRegressor(**lgb_params4),\n#            \"hgbm\": HistGradientBoostingRegressor(max_iter=self.n_estimators, learning_rate=0.01, loss=\"squared_error\", \n#                                                  n_iter_no_change=300,random_state=self.random_state),\n            'cat_reg': CatBoostRegressor(**cb_params),\n#            'cat_class1':CatBoostClassifier(**cat_class1),\n#            'cat_class2':CatBoostClassifier(**cat_class2),\n#             'cat_reg2': CatBoostRegressor(**cb_params1),\n#             'cat_reg3': CatBoostRegressor(**cb_params2),\n           \"cat_sym\": CatBoostRegressor(**cb_sym_params),\n#             \"cat_loss\": CatBoostRegressor(**cb_loss_params),\n#             'etr': ExtraTreesRegressor(min_samples_split=12, min_samples_leaf= 6, max_depth=10,\n#                                        n_estimators=500,random_state=self.random_state),\n #           \"GradientBoostingRegressor\": GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=6,loss=\"squared_error\", random_state=self.random_state),\n#             \"RandomForestRegressor\": RandomForestRegressor(max_depth= 6,max_features= 'auto',min_samples_split= 4,\n#                                                            min_samples_leaf= 4,  n_estimators=500, random_state=self.random_state, n_jobs=-1),\n#            'dt': DecisionTreeRegressor(**dt_params),\n#            \"lr\":LinearRegression(),\n# #             \"knn\":KNeighborsRegressor(**knn_params),\n#             \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(max_iter=3000, tol=1e-3, n_iter_no_change=60, random_state=self.random_state),\n            \"HuberRegressor\": HuberRegressor(max_iter=3000),\n#             'ann':ann,\n#             \"ann2\":ann2\n            \n            \n        }\n\n\n        return reg_models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Weighted Esembling ","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", -1, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n        weighted_pred=np.where(weighted_pred<1,1,weighted_pred)\n\n        # Calculate the RMSE score for the weighted prediction\n        score = rmse(np.log1p(y_true), np.log1p(weighted_pred))\n        return score\n\n    def fit(self, y_true, y_preds, n_trials=2000):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='minimize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds, n_trials=2000):\n        self.fit(y_true, y_preds, n_trials=n_trials)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:40.607091Z","iopub.status.idle":"2024-04-08T16:24:40.607568Z","shell.execute_reply.started":"2024-04-08T16:24:40.607355Z","shell.execute_reply":"2024-04-08T16:24:40.607373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Model Fit","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 10\nrandom_state = 42\nrandom_state_list = [42] \nn_estimators = 9999 \nearly_stopping_rounds = 600\nverbose = False\ndevice = 'cpu'\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n\n# Initialize an array for storing test predictions\ntest_predss = np.zeros(X_test.shape[0])\nensemble_score = []\nweights = []\ntrained_models = dict(zip(Regressor().reg_models.keys(), [[] for _ in range(Regressor().len_models)]))\ntrained_models = {'xgb_reg':[]}\n\n# Evaluate on validation data and store predictions on test data\nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n\n    # Get a set of Regressor models\n    reg = Regressor(n_estimators, device, random_state)\n    models = reg.reg_models\n\n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n\n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n            if ('lgb' in name) and (\"reg\" in name): #categorical_feature=cat_features\n                model.fit(X_train_, np.log1p(y_train_), eval_set=[(X_val, np.log1p(y_val))]),#,categorical_feature=cat_features,)\n            elif 'class' in name:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif \"xgb\" in name:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            else:\n                model.fit(X_train_, np.log1p(y_train_), eval_set=[(X_val, np.log1p(y_val))], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        elif name=='ann':\n            model.fit(X_train_, np.log1p(y_train_), validation_data=(X_val, np.log1p(y_val)),batch_size=50, epochs=200,verbose=verbose)\n        else:\n            model.fit(X_train_, np.log1p(y_train_))\n            \n        if name==\"ann\":\n            y_val_pred = np.expm1(model.predict(X_val)[:,0])\n            test_pred = np.expm1(model.predict(X_test)[:,0])\n        elif \"class\" in name:\n            y_val_pred =np.array(model.predict(X_val))[:,0]\n            test_pred = np.array(model.predict(X_test))[:,0]\n        elif \"xgb\" in name:\n            y_val_pred =np.array(model.predict(X_val))\n            test_pred = np.array(model.predict(X_test))            \n        else:\n            y_val_pred = np.expm1(model.predict(X_val))\n            test_pred = np.expm1(model.predict(X_test))\n\n\n#         # Convert predicted values back to their original scale by applying the expm1 function\n#         y_val_pred = np.expm1(y_val_pred)\n#         test_pred = np.expm1(test_pred)\n\n        score = rmse(np.log1p(y_val), np.log1p(y_val_pred))\n#         print(\"pure RMSE\",rmse(np.log1p(y_val), np.log1p(y_val_pred)))\n\n#         score = rmse(np.expm1(y_val), y_val_pred)\n\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] RMSLE score: {score:.5f}')\n\n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n\n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    \n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n#     y_val_pred = optweights.fit_predict(np.expm1(y_val.values), oof_preds)\n\n    score = rmse(np.log1p(y_val), np.log1p(y_val_pred))\n#     score = rmse(np.expm1(y_val), y_val_pred)\n\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] RMSLE score -------> {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:40.609205Z","iopub.status.idle":"2024-04-08T16:24:40.609596Z","shell.execute_reply.started":"2024-04-08T16:24:40.609411Z","shell.execute_reply":"2024-04-08T16:24:40.609427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble RMSLE score {mean_score:.5f} ± {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name} {mean_weight:.5f} ± {std_weight:.5f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:40.611051Z","iopub.status.idle":"2024-04-08T16:24:40.611982Z","shell.execute_reply.started":"2024-04-08T16:24:40.611718Z","shell.execute_reply":"2024-04-08T16:24:40.611737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 Feature Importance","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, top=15):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(top)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(12, 10))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color='brown', errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance [Top {top}]', fontsize=15)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-08T16:24:40.613451Z","iopub.status.idle":"2024-04-08T16:24:40.614288Z","shell.execute_reply.started":"2024-04-08T16:24:40.614071Z","shell.execute_reply":"2024-04-08T16:24:40.61409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. RESULT","metadata":{}},{"cell_type":"code","source":"submission[target] = test_predss\nsubmission[target]=np.clip(submission[target],1,29)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:40.615709Z","iopub.status.idle":"2024-04-08T16:24:40.616633Z","shell.execute_reply.started":"2024-04-08T16:24:40.616425Z","shell.execute_reply":"2024-04-08T16:24:40.616443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. GENERALIZATION","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">I'm taking my previous submissions collated in a data card and average the present result with previous results either by AM/GM/HM</font>","metadata":{}},{"cell_type":"code","source":"target=[target]\nsub_ext1=pd.read_csv(\"/kaggle/input/clean-code-voting-regressor-base-3-models/submission.csv\")\nsub_ext2=pd.read_csv(\"/kaggle/input/regression-with-an-abalone-dataset-ml-regressor/submission.csv\")\n\nsubmission1=pd.read_csv(\"/kaggle/input/ps4e4-ensemble-ancillary/sub_pure_14648.csv\")\nsubmission2=pd.read_csv(\"/kaggle/input/ps4e4-ensemble-ancillary/sub_pure_14651.csv\")\nsubmission3=pd.read_csv(\"/kaggle/input/ps4e4-ensemble-ancillary/sub_pure_14685.csv\")\n\n\nsub_list=[sub_ext1,sub_ext2, submission1,submission2, submission] # list all the results\n\n\"\"\"\nAssign Weights of each submissions, a general method is to rank them.\n\"\"\"\nweights=[3,3,1,1,3]\n\n\nweighted_list = [item for sublist, weight in zip(sub_list, weights) for item in [sublist] * weight]\n\ndef ensemble_mean(sub_list,cols, mean=\"AM\"):\n    \n    \"\"\"\n    The function computes Arithmetic Mean/Geometric Mean/Harmonic Mean given a list of results with specific results.\n    \"\"\"\n    \n    sub_out=sub_list[0].copy()\n    if mean==\"AM\":\n        for col in cols:\n            sub_out[col]=sum(df[col] for df in sub_list)/len(sub_list)\n    elif mean==\"GM\":\n        for df in sub_list[1:]:\n            for col in cols:\n                sub_out[col]*=df[col]\n        for col in cols:\n            sub_out[col]=(sub_out[col])**(1/len(sub_list))\n    elif mean==\"HM\":\n        for col in cols:\n            sub_out[col]=len(sub_list)/sum(1/df[col] for df in sub_list)\n    \n    return sub_out\n    \nsub_ensemble=ensemble_mean(weighted_list,target,mean=\"AM\")\nsub_ensemble.to_csv('submission_comb.csv',index=False)\nsub_ensemble.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:24:40.617993Z","iopub.status.idle":"2024-04-08T16:24:40.61888Z","shell.execute_reply.started":"2024-04-08T16:24:40.61863Z","shell.execute_reply":"2024-04-08T16:24:40.618649Z"},"trusted":true},"execution_count":null,"outputs":[]}]}