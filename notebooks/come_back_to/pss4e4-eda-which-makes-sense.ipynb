{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":74776,"databundleVersionId":8172961,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abalone age estimation: EDA which makes sense\nhttps://www.kaggle.com/competitions/surrogate-playground-series-s4e4\nThis model analyzes the competition data and compares the performance of a few models with tuned hyperparameters. It furthermore shows how to ensemble the models.\n\nYou can use exactly the same notebook to participate in either one of two competitions:\n1. The official playground competition: [Regression with an Abalone Dataset](https://www.kaggle.com/competitions/playground-series-s4e4)\n2. @tilii's [Surrogate Playground Series S4E4](https://www.kaggle.com/competitions/surrogate-playground-series-s4e4) announced in [this post](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/492170)\n\n**This version of the notebook is configured for the surrogate competition. If you want to use it for the official competition, set `OFFICIAL_COMPETITION` to True.** If you just want to see the cross-validation results for the official competition, look at [version 6](https://www.kaggle.com/code/ambrosm/pss4e4-eda-which-makes-sense?scriptVersionId=170756404) of the notebook.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom colorama import Fore, Style\nimport xgboost\nimport lightgbm\nimport catboost\nimport os\n\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold, StratifiedKFold \nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import TransformedTargetRegressor, ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, SplineTransformer\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-10T18:27:03.903548Z","iopub.execute_input":"2024-04-10T18:27:03.903923Z","iopub.status.idle":"2024-04-10T18:27:08.201554Z","shell.execute_reply.started":"2024-04-10T18:27:03.903893Z","shell.execute_reply":"2024-04-10T18:27:08.200575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration\n# Produce a submission file (you can set this to false if you only\n# want to see the cross-validation results)\nCOMPUTE_TEST_PRED = True\n\n# Participate in the official competition or the surrogate competition\nOFFICIAL_COMPETITION = False\n\n# Add the original data source to the synthetic data for training\n# Recommendation for the official competition: set USE_ORIGINAL_DATA = True\n# Recommendation for the surrogate competition: set USE_ORIGINAL_DATA = False\nUSE_ORIGINAL_DATA = OFFICIAL_COMPETITION\n\n# Containers for results\noof, test_pred = {}, {}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:08.203169Z","iopub.execute_input":"2024-04-10T18:27:08.203689Z","iopub.status.idle":"2024-04-10T18:27:08.209082Z","shell.execute_reply.started":"2024-04-10T18:27:08.20366Z","shell.execute_reply":"2024-04-10T18:27:08.208032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the data\n\nWe read the data files and observe the following:\n- The sex of these animals is either male, female or infant. All other features are numeric.\n- All feature values end in 0 or 5 - this is typical for measurements with finite precision.\n- The target variable, Rings, is integer.\n\n**Insight:**\n- As the target variable is integer, we can use it to cross-validate with a `StratifiedKFold`. The stratification gives a better cvâ€“lb correlation than a simple `KFold`.\n- With >90000 samples, the training dataset is too big for kernel-based regressors (e.g., `SVR`, `KernelRidge`, `GaussianProcessRegressor`).\n- `Sex` is a categorical feature. For some models, we'll need to one-hot encode it, for other models it suffices to mark it as categorical.","metadata":{}},{"cell_type":"code","source":"if OFFICIAL_COMPETITION:\n    train = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv', index_col='id')\nelse:\n    train = pd.read_csv('/kaggle/input/surrogate-playground-series-s4e4/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/surrogate-playground-series-s4e4/test.csv', index_col='id')\n    \ntrain['Sex'] = train.Sex.astype('category')\ntest['Sex'] = test.Sex.astype(train.Sex.dtype)\n\nif not train.isna().any().any():\n    print('There are no missing values in train.')\nif not test.isna().any().any():\n    print('There are no missing values in test.')\n    \nprint(f\"Train shape: {train.shape}   test shape: {test.shape}\")\n    \nnumeric_features = ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight']\nnumeric_vars = numeric_features + ['Rings']\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:08.210343Z","iopub.execute_input":"2024-04-10T18:27:08.210613Z","iopub.status.idle":"2024-04-10T18:27:08.534661Z","shell.execute_reply.started":"2024-04-10T18:27:08.210591Z","shell.execute_reply":"2024-04-10T18:27:08.533627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can add the original dataset to the synthetic data for training. Although with only 4177 samples it is twenty times smaller than the competition dataset, its addition improves the scores of all models.\n\nThanks to @iqbalsyahakbar [for the code](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/488082).","metadata":{}},{"cell_type":"code","source":"if USE_ORIGINAL_DATA:\n    !pip install ucimlrepo\n\n    from ucimlrepo import fetch_ucirepo \n\n    # fetch dataset \n    abalone = fetch_ucirepo(id=1) \n\n    # data (as pandas dataframes)\n    original_dataset = pd.concat([abalone.data.features, abalone.data.targets], axis = 1).rename({\n        'Whole_weight' : 'Whole weight',\n        'Shucked_weight' : 'Whole weight.1',\n        'Viscera_weight' : 'Whole weight.2',\n        'Shell_weight' : 'Shell weight'\n    }, axis = 1)\n    original_dataset['Sex'] = original_dataset.Sex.astype(train.Sex.dtype)\n    print(\"Original dataset shape:\", original_dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:08.536891Z","iopub.execute_input":"2024-04-10T18:27:08.537162Z","iopub.status.idle":"2024-04-10T18:27:08.543976Z","shell.execute_reply.started":"2024-04-10T18:27:08.53714Z","shell.execute_reply":"2024-04-10T18:27:08.542953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation\n\nAll features are highly correlated to the target variable. This is very plausible: The older an individual abalone gets, the higher its weight, length, and so on.\n\n**Insight:**\n- There is no need to downselect features. All features are useful for predicting the target.\n- We don't need to look at feature importances. All features are important.","metadata":{}},{"cell_type":"code","source":"cc = np.corrcoef(train[numeric_vars], rowvar=False)\nsns.heatmap(cc, center=0, cmap='coolwarm', annot=True,\n            xticklabels=numeric_vars, yticklabels=numeric_vars)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:08.545175Z","iopub.execute_input":"2024-04-10T18:27:08.545566Z","iopub.status.idle":"2024-04-10T18:27:09.02211Z","shell.execute_reply.started":"2024-04-10T18:27:08.54554Z","shell.execute_reply":"2024-04-10T18:27:09.021007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target distribution","metadata":{}},{"cell_type":"code","source":"vc = train.Rings.value_counts()\nplt.figure(figsize=(6, 2))\nplt.bar(vc.index, vc)\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:42.401405Z","iopub.execute_input":"2024-04-10T18:27:42.401995Z","iopub.status.idle":"2024-04-10T18:27:42.559518Z","shell.execute_reply.started":"2024-04-10T18:27:42.401965Z","shell.execute_reply":"2024-04-10T18:27:42.558679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- Because all training targets are between 1 and 29, we may clip all predictions to the interval \\[1, 29\\].","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering\n\nBecause the competition metric is RMSLE, all our models will predict the logarithm of the target. In this situation, some models will perform better if we feed them the logarithm of the features. ","metadata":{}},{"cell_type":"code","source":"log_features = []\nfor col in numeric_features:\n    train[f'log_{col}'] = np.log1p(train[col])\n    test[f'log_{col}'] = np.log1p(test[col])\n    if USE_ORIGINAL_DATA:\n        original_dataset[f'log_{col}'] = np.log1p(original_dataset[col])\n    log_features.append(f'log_{col}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:09.235291Z","iopub.execute_input":"2024-04-10T18:27:09.236192Z","iopub.status.idle":"2024-04-10T18:27:09.269084Z","shell.execute_reply.started":"2024-04-10T18:27:09.236156Z","shell.execute_reply":"2024-04-10T18:27:09.268335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nTo ensure that our cross-validation results are consistent, we'll use the same function for cross-validating all models.\n\nNotice that in cross-validation, we first split the dataset and then add the original data only to the training dataset. The validation dataset consists purely of competition data. This setup lets us correctly assess whether the original data are useful or harmful.","metadata":{}},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\ndef cross_validate(model, label, features=test.columns, n_repeats=1):\n    \"\"\"Compute out-of-fold and test predictions for a given model.\n    \n    Out-of-fold and test predictions are stored in the global variables\n    oof and test_pred, respectively.\n    \n    If n_repeats > 1, the model is trained several times with different seeds.\n    \n    All predictions are clipped to the interval [1, 29].\n    \"\"\"\n    scores = []\n    oof_preds = np.full_like(train.Rings, np.nan, dtype=float)\n    for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.Rings)):\n        X_tr = train.iloc[idx_tr][features]\n        X_va = train.iloc[idx_va][features]\n        y_tr = train.iloc[idx_tr].Rings\n        y_va = train.iloc[idx_va].Rings\n        \n        if USE_ORIGINAL_DATA:\n            X_tr = pd.concat([X_tr, original_dataset[features]], axis=0)\n            y_tr = pd.concat([y_tr, original_dataset.Rings], axis=0)\n            \n        y_pred = np.zeros_like(y_va, dtype=float)\n        for i in range(n_repeats):\n            m = clone(model)\n            if n_repeats > 1:\n                mm = m\n                if isinstance(mm, Pipeline):\n                    mm = mm[-1]\n                if isinstance(mm, TransformedTargetRegressor):\n                    mm = mm.regressor\n                mm.set_params(random_state=i)\n            m.fit(X_tr, y_tr)\n            y_pred += m.predict(X_va)\n        y_pred /= n_repeats\n        y_pred = y_pred.clip(1, 29)\n        \n#         residuals = np.log1p(y_va) - np.log1p(y_pred)\n#         plt.figure(figsize=(6, 2))\n#         plt.scatter(y_pred, residuals, s=1)\n#         plt.axhline(0, color='k')\n#         plt.show()\n        \n        score = mean_squared_log_error(y_va, y_pred, squared=False)\n        print(f\"# Fold {fold}: RMSLE={score:.5f}\")\n        scores.append(score)\n        oof_preds[idx_va] = y_pred\n    print(f\"{Fore.GREEN}# Overall: {np.array(scores).mean():.5f} {label}{Style.RESET_ALL}\")\n    oof[label] = oof_preds\n    \n    if COMPUTE_TEST_PRED:\n        # Retrain n_repeats times with the whole dataset and average\n        y_pred = np.zeros(len(test), dtype=float)\n        X_tr = train[features]\n        y_tr = train.Rings\n        if USE_ORIGINAL_DATA:\n            X_tr = pd.concat([X_tr, original_dataset[features]], axis=0)\n            y_tr = pd.concat([y_tr, original_dataset.Rings], axis=0)\n        for i in range(n_repeats):\n            m = clone(model)\n            if n_repeats > 1:\n                mm = m\n                if isinstance(mm, Pipeline):\n                    mm = mm[-1]\n                if isinstance(mm, TransformedTargetRegressor):\n                    mm = mm.regressor\n                mm.set_params(random_state=i)\n            m.fit(X_tr, y_tr)\n            y_pred += m.predict(test[features])\n        y_pred /= n_repeats\n        y_pred = y_pred.clip(1, 29)\n        test_pred[label] = y_pred\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:09.270408Z","iopub.execute_input":"2024-04-10T18:27:09.270698Z","iopub.status.idle":"2024-04-10T18:27:09.284575Z","shell.execute_reply.started":"2024-04-10T18:27:09.270673Z","shell.execute_reply":"2024-04-10T18:27:09.283616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n\nWe compare several models. As the competition requires us to optimize the mean squared log error, we use a `TransformedTargetRegressor`. If a raw model optimizes the mean squared error (MSE), its `TransformedTargetRegressor` will optimize the mean squared log error.\n\nWe start with two linear models (ridge regression). For these models, we need to one-hot encode the categorical feature and to scale all features. The linear models improve if we add some nonlinear features with either `PolynomialFeatures` or `Nystroem`.","metadata":{}},{"cell_type":"code","source":"# PolynomialFeatures + Ridge\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      PolynomialFeatures(degree=3),\n                      TransformedTargetRegressor(Ridge(100),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Poly-Ridge', numeric_features + log_features + ['Sex'])\n# Overall: 0.15293 Poly-Ridge","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:09.285779Z","iopub.execute_input":"2024-04-10T18:27:09.286501Z","iopub.status.idle":"2024-04-10T18:27:24.528494Z","shell.execute_reply.started":"2024-04-10T18:27:09.286474Z","shell.execute_reply":"2024-04-10T18:27:24.526205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nystroem transformer + Ridge\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      Nystroem(n_components=500),\n                      TransformedTargetRegressor(Ridge(0.1),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Nystroem-Ridge', numeric_features + log_features + ['Sex'])\n# Overall: 0.15159 Nystroem-Ridge","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.529376Z","iopub.status.idle":"2024-04-10T18:27:24.530115Z","shell.execute_reply.started":"2024-04-10T18:27:24.529935Z","shell.execute_reply":"2024-04-10T18:27:24.529951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`KNeighborsRegressor` seems to be the worst of all models I looked at for this competition: ","metadata":{}},{"cell_type":"code","source":"# K nearest neighbors\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      StandardScaler(),\n                      TransformedTargetRegressor(KNeighborsRegressor(n_neighbors=50),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'KNN', log_features + ['Sex'])\n# Overall: 0.15458 KNN\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.531244Z","iopub.status.idle":"2024-04-10T18:27:24.531718Z","shell.execute_reply.started":"2024-04-10T18:27:24.531544Z","shell.execute_reply":"2024-04-10T18:27:24.531559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random forest\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(RandomForestRegressor(n_estimators=200, min_samples_leaf=8, max_features=5),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'Random forest', log_features + ['Sex'])\n# Overall: 0.14962 Random forest","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.532763Z","iopub.status.idle":"2024-04-10T18:27:24.53367Z","shell.execute_reply.started":"2024-04-10T18:27:24.533483Z","shell.execute_reply":"2024-04-10T18:27:24.5335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ExtraTreesRegressor\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(drop='first'), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(ExtraTreesRegressor(n_estimators=200, min_samples_leaf=7),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'ExtraTrees', log_features + ['Sex'])\n# Overall: 0.15048 ExtraTrees","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.534722Z","iopub.status.idle":"2024-04-10T18:27:24.535046Z","shell.execute_reply.started":"2024-04-10T18:27:24.53489Z","shell.execute_reply":"2024-04-10T18:27:24.534903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HistGradientBoostingRegressor\n# Hyperparameters were tuned with Optuna\nhgb_params = {'max_iter': 300, 'max_leaf_nodes': 43, 'early_stopping': False, 'learning_rate': 0.08019987638525192, 'min_samples_leaf': 37} # 0.14916\nmodel = make_pipeline(ColumnTransformer([('ohe', OneHotEncoder(), ['Sex'])],\n                                        remainder='passthrough'),\n                      TransformedTargetRegressor(HistGradientBoostingRegressor(**hgb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1))\ncross_validate(model, 'HGB', numeric_features + ['Sex'])\n# Overall: 0.14925 HGB","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.536173Z","iopub.status.idle":"2024-04-10T18:27:24.536515Z","shell.execute_reply.started":"2024-04-10T18:27:24.53635Z","shell.execute_reply":"2024-04-10T18:27:24.536364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We implement two XGBoost models as suggested in @siukeitin's discussion [log1p-transformed target + MSE objective vs MSLE objective](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/488283).\n- The first XGBoost model optimizes MSE and has a log-transformed target.\n- The second XGBoost model optimizes MSLE directly and doesn't need a target transformation.","metadata":{}},{"cell_type":"code","source":"# XGBoost with RMSE objective\n# Hyperparameters were tuned with Optuna\nxgb_params = {'grow_policy': 'lossguide', 'n_estimators': 300, 'learning_rate': 0.09471805900675286, 'max_depth': 8, 'reg_lambda': 33.33929116223339, 'min_child_weight': 27.048028004026204, 'colsample_bytree': 0.6105442825961575, 'objective': 'reg:squarederror', 'tree_method': 'hist', 'gamma': 0, 'enable_categorical': True} # 0.14859\nmodel = TransformedTargetRegressor(xgboost.XGBRegressor(**xgb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'XGBoost', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14853 XGBoost","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.53763Z","iopub.status.idle":"2024-04-10T18:27:24.537947Z","shell.execute_reply.started":"2024-04-10T18:27:24.537791Z","shell.execute_reply":"2024-04-10T18:27:24.537805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost with RMSLE objective\n# Hyperparameters were tuned with Optuna\nxgb_params = {'grow_policy': 'depthwise', 'n_estimators': 500, 'learning_rate': 0.0896765799823656, 'max_depth': 8, 'reg_lambda': 1.003764844090402, 'min_child_weight': 0.20627702562667777, 'colsample_bytree': 0.5142803343048419, 'objective': 'reg:squaredlogerror', 'tree_method': 'hist', 'gamma': 0, 'enable_categorical': True} # 0.14875\nmodel = xgboost.XGBRegressor(**xgb_params)\ncross_validate(model, 'XGBoost-RMSLE', numeric_features + ['Sex'], n_repeats=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.539184Z","iopub.status.idle":"2024-04-10T18:27:24.539596Z","shell.execute_reply.started":"2024-04-10T18:27:24.539422Z","shell.execute_reply":"2024-04-10T18:27:24.539442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM\n# Hyperparameters were tuned with Optuna\nlgbm_params = {'n_estimators': 1000, 'learning_rate': 0.038622511348472645, 'colsample_bytree': 0.5757189042456357, 'reg_lambda': 0.09664116733307193, 'min_child_samples': 87, 'num_leaves': 43, 'verbose': -1} # 0.14804\nmodel = TransformedTargetRegressor(lightgbm.LGBMRegressor(**lgbm_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'LightGBM', numeric_features + ['Sex'], n_repeats=5)\n# Overall: 0.14804 LightGBM\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.541248Z","iopub.status.idle":"2024-04-10T18:27:24.541581Z","shell.execute_reply.started":"2024-04-10T18:27:24.541416Z","shell.execute_reply":"2024-04-10T18:27:24.541436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There [was a discussion](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/491703) about whether a sqrt transformation of the target was better than the log transformation. The following LightGBM model uses the sqrt transformation and has a higher error than the model above. Log transformation is the way to go!","metadata":{}},{"cell_type":"code","source":"# LightGBM with sqrt transformation\n# Hyperparameters were tuned with Optuna\nlgbm_params = {'n_estimators': 1000, 'learning_rate': 0.038622511348472645, 'colsample_bytree': 0.5757189042456357, 'reg_lambda': 0.09664116733307193, 'min_child_samples': 87, 'num_leaves': 43, 'verbose': -1} # 0.14804\nmodel = TransformedTargetRegressor(lightgbm.LGBMRegressor(**lgbm_params),\n                                                 func=np.sqrt,\n                                                 inverse_func=np.square)\ncross_validate(model, 'LightGBM-sqrt', numeric_features + ['Sex'], n_repeats=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.542442Z","iopub.status.idle":"2024-04-10T18:27:24.543066Z","shell.execute_reply.started":"2024-04-10T18:27:24.54289Z","shell.execute_reply":"2024-04-10T18:27:24.542906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Catboost\n# Hyperparameters were tuned with Optuna\ncb_params = {'grow_policy': 'SymmetricTree', 'n_estimators': 1000, 'learning_rate': 0.128912681527133, 'l2_leaf_reg': 1.836927907521674, 'max_depth': 6, 'colsample_bylevel': 0.6775373040510968, 'random_strength': 0, 'boost_from_average': True, 'loss_function': 'RMSE', 'cat_features': ['Sex'], 'verbose': False} # 0.14847\nmodel = TransformedTargetRegressor(catboost.CatBoostRegressor(**cb_params),\n                                                 func=np.log1p,\n                                                 inverse_func=np.expm1)\ncross_validate(model, 'Catboost', log_features + ['Sex'], n_repeats=5)\n# Overall: 0.14851 Catboost","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.543904Z","iopub.status.idle":"2024-04-10T18:27:24.544232Z","shell.execute_reply.started":"2024-04-10T18:27:24.544061Z","shell.execute_reply":"2024-04-10T18:27:24.544074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble\n\n`Ridge` is always a good method to ensemble regression models. It determines the optimal weights for blending the ensemble members, as shown in the pie chart.\n\nNote that I haven't tuned the regularization `alpha` of the regressor. If you modify the code so that you can cross-validate the ensemble and tune the hyperparameter, you'll get a better ensemble score.","metadata":{}},{"cell_type":"code","source":"members = [name for name in oof.keys() if 'Stack' not in name]\n\nX = np.log1p(np.column_stack([oof[name] for name in members]))\nmodel = TransformedTargetRegressor(Ridge(positive=True, tol=1e-6),\n                                   func=np.log1p,\n                                   inverse_func=np.expm1)\nmodel.fit(X, train.Rings)\nprint('Ensemble weights')\nweights = pd.Series(model.regressor_.coef_, index=members)\nprint(weights)\nprint('Total weight:', weights.sum())\nprint('Intercept:', model.regressor_.intercept_)\noof['Stack'] = model.predict(X) # not really out-of-fold...\nprint(f\"Score: {mean_squared_log_error(train.Rings, oof['Stack'], squared=False):.5f}\")\n\n# Pie chart\nweights = weights[weights >= 0.005] # hide small weights in pie chart\nplt.pie(weights, labels=weights.index, autopct=\"%.0f%%\")\nplt.title('Ensemble weights')\nplt.show()\n\n# Test predictions\nif COMPUTE_TEST_PRED:\n    X = np.log1p(np.column_stack([test_pred[name] for name in members]))\n    test_pred['Stack'] = model.predict(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.545338Z","iopub.status.idle":"2024-04-10T18:27:24.545668Z","shell.execute_reply.started":"2024-04-10T18:27:24.545508Z","shell.execute_reply":"2024-04-10T18:27:24.545522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\n\nThe bar chart shows that for the given dataset, LightGBM, Catboost and XGBoost give the best predictions. The ensemble ('Stack') outperforms every single model.","metadata":{}},{"cell_type":"code","source":"result_list = []\nfor label in oof.keys():\n    score = mean_squared_log_error(train.Rings, oof[label], squared=False)\n    result_list.append((label, score))\nresult_df = pd.DataFrame(result_list, columns=['label', 'score'])\nresult_df.sort_values('score', inplace=True)\n\nplt.figure(figsize=(12, len(result_df) * 0.4 + 0.4))\nbars = plt.barh(np.arange(len(result_df)), result_df.score, color='lightgreen')\nplt.gca().bar_label(bars, fmt='%.5f')\nplt.yticks(np.arange(len(result_df)), result_df.label)\nplt.gca().invert_yaxis()\nif OFFICIAL_COMPETITION:\n    plt.xlim(0.14, 0.16)\nelse:\n    plt.xlim(0.16, 0.17)\nplt.xlabel('Root mean squared log error (lower is better)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-10T18:27:24.546602Z","iopub.status.idle":"2024-04-10T18:27:24.546917Z","shell.execute_reply.started":"2024-04-10T18:27:24.54676Z","shell.execute_reply":"2024-04-10T18:27:24.546773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"if COMPUTE_TEST_PRED:\n    sub = pd.Series(test_pred['Stack'], index=test.index, name='Rings')\n    filename = 'submission.csv' if OFFICIAL_COMPETITION else 'submission_surrogate.csv'\n    sub.to_csv(filename)\n    os.system(f\"head {filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:27:24.547741Z","iopub.status.idle":"2024-04-10T18:27:24.548479Z","shell.execute_reply.started":"2024-04-10T18:27:24.548295Z","shell.execute_reply":"2024-04-10T18:27:24.54831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exercises\n\nBy completing at least one of the following exercises, you have a good chance of improving your leaderboard score:\n1. Re-run the whole notebook with another seed for the StratifiedKFold to test for overfitting. How much do the cv scores and the ensemble weights depend on the seed?\n1. Improve the ensemble by tuning the regularization and by manually excluding models.\n1. Implement and tune a LightGBM model with RMSLE objective (and without target transformation) as described in @siukeitin's discussion [log1p-transformed target + MSE objective vs MSLE objective](https://www.kaggle.com/competitions/playground-series-s4e4/discussion/488283). Add it to the ensemble.\n1. Implement and tune a neural network and add it to the ensemble.","metadata":{}}]}